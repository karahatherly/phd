\chapter{Implementation in seL4}
\label{chap:implementation}

In the previous chapter we presented our mechanisms for temporal isolation and safe resource sharing
in a high-assurance microkernel. 
Now we delve into the implementation details of scheduling contexts, scheduling context donation,
timeout fault handlers, and IPC forwarding in \selfour.

First we survey new and changed kernel objects, followed by the updated system call API, and ...
\TODO{update when done}.

\section{Objects}

We add two new objects to the kernel, \emph{\glspl{SCO}} and \emph{resume objects}. Additionally, we modify
the \gls{TCB} object although do not increase its total size. Finally, we modify the notification
object to allow \glspl{SCO} to allow single-threaded, passive servers to receive signals in
addition to IPC messages.

\subsection{Resume objects}
\label{s:resume}

Resume objects, modelled after KeyKOS~\citep{Bomberger_FFHLS_92}, are
a new object type that generalise
``reply capabilities'' of baseline \selfour introduced in \cref{s:ipc}.
Recall that in baseline \selfour, the receiver of the message (i.e.\ the
server) receives the reply capability in a magic ``reply slot'' in its
capability space. The server replies by invoking that
capability. Resume objects remove the magic by explicitly representing
the reply channel, and servers with multiple clients can dedicate a resume object per client.
Instead of generating a one-shot reply capability in a reply slot on a \call, the operation
populates a resume object, while \recv de-populates the resume object. 
They also
provide more efficient support for stateful servers that handle
concurrent client sessions, which we expand on further when we introduce the changed system call \gls{API} in \cref{s:new-api}.

Resume objects are small (16 bytes on 32-bit platforms, and 32 bytes on 64-bit platforms), and
contain the fields shown in \cref{tab:reply_object}.

\begin{table}[h]
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{lX}\toprule
        \emph{Field} & \emph{Description} \\\midrule
        \code{tcb}    & The calling or waiting thread that is blocked on this reply object. \\
        \code{prev} & \code{NULL} if this is the start of the call stack, otherwise points to the previous
        reply object in the call stack. \\
        \code{next} & Either a pointer to the scheduling context that was last donated using this
        reply object, if this reply object is the head of a call stack (the last caller before the
        server) or a pointer to the next reply object in the stack. 0 if no scheduling context was
        passed along the stack.\\\bottomrule
    \end{tabularx}
    \caption{Fields in a reply object.}
    \label{tab:reply_object}
\end{table}

There are three states a resume object can be in: idle, meaning it is not currently being used,
waiting, meaning it is being used by a thread blocked and waiting for a message, or active,
which means the resume object currently has a thread blocked waiting for a reply associated with it.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{resume}
    \caption{State diagram of resume objects.}
    \label{f:resume-state-diagram}
\end{figure}

Valid state transitions are shown in \cref{f:resume-state-diagram}. Resume objects are provided to
\code{recv} system calls along with endpoint capabilities, which transitions them from idle to
waiting.
If the endpoint is invoked with a \code{call}, the caller is blocked on the resume capability, and
it transitions to active.  
If a resume object is directly invoked, using a sending system call (recall from
\cref{sec:sel4-system-call-and-invocations} that sending system calls are \code{send},
\code{nbsend}, \code{call}, \code{reply}) then a reply message is delivered to the thread blocked on
the object. Finally, if a resume object is in an active state, and provided to \code{recv}, the object
is first invoked, and removed from the call stack, which we now examine in detail. 

\subsubsection{The call stack}

Active resume objects track the \emph{call stack} that is built as nested \call operations take
place. A \call triggers to a push operation, adding to the top of the stack, and a reply message a
pop, removing the top of the stack.  The call stack allows us to track the path of a donated
scheduling context, from caller to callee, so that it can be returned to the previous caller
regardless of which thread sends the reply message. This is a direct consequence of flexible resume
capabilities: resume capability can be moved between threads, and \emph{any} thread can execute the
reply: usually the server, but occasionally a timeout fault handler, or a nested server which
received the resume capability via \gls{IPC} forwarding. It is therefore impossible to derive the
original callee from the thread sending the reply message.

The call stack is structured as a doubly-linked list, with one minor difference: the head of the
call stack is the scheduling context that was donated along the stack, which itself contains a
pointer to the thread currently executing on it. Each resume object then forms a node in the
stack, going back to the original caller at the base. The \gls{SCO} remains the head of the stack
until the \gls{SCO} returns to the initial caller and the stack is fully dismantled.  When a reply
message is sent, the scheduling context travels back along the call stack and the head resume object
is popped.  Reply objects also point to the thread which donated the \gls{SCO} along the stack,
allowing the SCO to be returned to that thread when a reply message is sent.  This process is
illustrated in \cref{f:reply-stack}, where $A$ has called $S_{1}$ which has called  $S_{2}$.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{replychain}
    \caption{The state of the reply stack when a nested server $S_{2}$ is
performing a request on behalf of an initial server $S_{1}$ for client $A$.}
    \label{f:reply-stack}
\end{figure}

In a capability system, one of the biggest challenges is that capabilities can be deleted at any
time. The deletion semantics of the call stack are as follows: 
\begin{itemize}
    \item If the \gls{SCO} is deleted, the head of the call stack becomes the first resume object. To avoid
        a long-running operation, the call stack of resume objects remains linked, and is dismantled
        as each resume object is deleted.
    \item If the head resume object (the object that points to the SC) is deleted, the \gls{SCO} is
        returned along the stack to the caller. 
    \item If a resume object in the middle of the stack is deleted, we use the standard operation to
        remove a node from a doubly-linked list: the previous node is connected to the next node,
        and the deleted object is no longer pointed to by any member of the stack.
    \item If the resume object at the start of the stack, the node corresponding to the initial
        caller, it is simply removed. The \gls{SCO} cannot return to the initial caller by means of a reply
        message.
\end{itemize}
    
\subsection{Scheduling context objects}
\label{s:sco}

We introduce a new kernel object type, \glspl{SCO}, which all processing time is accounted against, 
and a new scheduler invariant: any thread in the scheduler must have a \gls{SCO}. 
\glspl{SCO} are variable sized objects that represent access to a certain amount of time and
consist of a core amount of fields, and a circular buffer to track the consumption of time.
Scheduling contexts encapsulate processor time reservations,
derived from sporadic task parameters: minimum inter-arrival time ($T$) and a set of replenishments which is
populated from an original execution budget ($C$), representing the reserved rate
($U$) = $\frac{C}{T}$.
Fields in an \glspl{SCO} are shown in \cref{t:sc-fields}.

\begin{table}[h] 
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{lX}\toprule
        \emph{Field}   & \emph{Description}\\\midrule
        \code{Period}  & The replenishment period, $T$. \\
     \code{Consumed} & The amount of cycles consumed since the last reset. \\
     \code{Core}     & The ID of the processor this scheduling context grants time on.\\
     \code{TCB}      & A pointer to the thread (if any) that this scheduling context is
        currently providing time to.\\
     \code{Reply}    & A pointer to the last resume object in a call stack
        which this \gls{SCO} was donated along.\\
     \code{Notification} & A pointer (if any) to the notification object this \gls{SCO} is bound
        to.\\
     \code{Badge} & A unique identifier for this scheduling context, which is delivered as part of a
        timeout fault message to identify the faulting client.\\
     \code{YieldFrom} & Pointer to a \gls{TCB} that has performed a directed yield to this
        \gls{SCO}.\\
     \code{Head,Tail} & Indexes to the circular buffer of sporadic replenishments.\\
     \code{Max} & Size of the circular buffer of replenishments for this scheduling context.\\
        \bottomrule
    \end{tabularx}
    \caption{Fields of a scheduling context object.}
    \label{t:sc-fields}
\end{table}

\subsubsection{Replenishments}

In addition to the core fields, \glspl{SCO} contain a variable amount of \emph{replenishments},
which consist of a \code{timestamp} and \code{amount}. These are used for both round-robin and
sporadic threads. For round-robin threads we simply use the head replenishment to track how much
time is left in that \glspl{SCO} timeslice. 

Sporadic threads are more complex; the amount of replenishments is limited by both a variable
provided on configuration and the size of the \gls{SCO}, configured on creation. This allows system
designers to control the preemption and fragmentation of sporadic replenishments as discussed in
\cref{sec:model-sporadic}.

Each \gls{SCO} is minimum $2^{8}$ bytes in size, which can hold eight or ten replenishments for 32
and 64-bit processors respectively. This is sufficient for most uses, but more replenishments can
be supported by larger sizes, allowing system designers to trade-off latency for fragmentation
overhead in the sporadic server implementation. Scheduling contexts can be created as larger
objects, as any power of two, then the rest of the object can be filled with replenishments, as
shown in \cref{t:impl-sc-layout}. System designers can then use the \code{max} replenishment field
to specify exact the number of replenishment to use, up to the object size. 

\begin{table}[t] 
    \centering
    \begin{tabular}{c|c|c|c|c|}\cline{2-5}
        0x00 &  \multicolumn{2}{c}{\texttt{period}} & \multicolumn{2}{|c|}{\texttt{consumed}}
        \\\cline{2-5}
        0x10 & \texttt{core}                         & \texttt{TCB} & \texttt{reply} & \texttt{ntfn} \\\cline{2-5}
        0x20 &\texttt{badge}                        & \texttt{yieldFrom}                               & \texttt{head}  & \texttt{tail} \\\cline{2-5}
        0x30 & \texttt{max}                          &                                                  &                & \\\cline{2-5}
        0x40 & \multicolumn{4}{c|}{\texttt{replenishment$_{0}$}}  \\\cline{2-5}
        0x50 & \multicolumn{4}{c|}{\texttt{replenishment$_{1}$}}  \\\cline{2-5}
        \ldots & \multicolumn{4}{c|}{\ldots}  \\\cline{2-5}
        0xF0 & \multicolumn{4}{c|}{\texttt{replenishment$_{n}$}}  \\\cline{2-5}


    \end{tabular}
    \caption{Layout of a scheduling context object on a 32-bit system.}
    \label{t:impl-sc-layout}
\end{table}

\subsubsection{Admission}

Like any \selfour object, scheduling contexts are created from zeroed, untyped memory. Consequently,
new scheduling objects do not grant authority to any processing time at all, as the parameters are
all set to zero. To configure a scheduling context, the new \code{sched\_control} capability must be
invoked, which allows the period, initial sporadic replenishment, maximum number of refills, and
badge fields to be set. The processing core is derived from the \code{sched\_control} capability
that is invoked: only one exists per core. 

\subsection{Thread control blocks}

Recall from \cref{sec:sel4-tcb} that \glspl{TCB} are the abstraction of an execution context in
\selfour, which are formed from a TCB data structure and a CNode containing capabilities specific to
that thread. We make several alterations to this structure, but do not impact the \gls{TCB} size, as
there was enough space available. The altered fields are shown in \cref{t:tcb-fields}.

\begin{table}[h] 
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{lX}\toprule
        \emph{Field}   & \emph{Description}\\\midrule
        \sout{\code{timeslice}} & Used to track the timeslice, replaced by the scheduling context. \\
        \code{scheduling context} & The scheduling context (if any) this \gls{TCB} consumes time from. \\
        \code{MCP} & The \gls{MCP} of this \gls{TCB}. \\
        \code{reply} & A pointer to the resume object this TCB is blocked on, if the TCB is
        \code{BlockedOnReply} or \code{BlockedOnRecv}. \\
        \code{yieldTo} & Pointer to the \gls{SCO} this \gls{TCB} has performed a directed yield to,
        if any.\\
        \sout{\code{faultEndpoint}} &  Moved to the TCB CNode. \\
        \bottomrule
    \end{tabularx}
    \caption{Added and removed fields in a \gls{TCB} object.}
    \label{t:tcb-fields}
\end{table}


\subsubsection{Fault endpoints}

We also change the contents of the TCB CNode, removing two slots previously required by the reply
capability implementation, as resume objects are now provided to \code{recv()} system calls, and
no slot in the TCB CNode is required. Additionally, we change the behaviour of fault handlers: in
baseline seL4, the fault handling cap is looked up on every single fault, increasing the overhead of
fault handling. To minimise this overhead, we look up the fault endpoint when it is configured, and
install it in the TCB CNode. We do the same for timeout fault endpoints. 

\subsubsection{Maximum controlled priorities}

The MCP, or \emph{maximum controlled priority}, resurrects a concept from early L4
kernels~\citep{Liedtke_96:rm}. It supports lightweight, limited manipulation of thread priorities,
useful e.g.\ for implementing user-level thread packages. When setting
the priority or MCP of a TCB, \(A\),
the caller must provide the capability to a TCB, \(B\), (which could be the caller's
TCB). The caller is allowed to set the priority or MCP of \(A\) up to the value
of \(B\)'s MCP.\footnote{Obviously, this operation also requires a capability to \(A\)'s TCB.}
In a typical system, most threads
will run with an MCP of zero and have no access to TCB capabilities with a higher MCP, meaning they
cannot raise any thread's priority.
The MCP is taken from an explicitly-provided TCB, rather than the caller's, to avoid the
confused deputy problem~\citep{Hardy_88}.

\subsection{Notification objects}

A scheduling context object can be associated with notification object, which allows a passive
servers with bound notifications to receive signals and run on the notification's
scheduling context to process those signals.
We add a pointer to the scheduling context from the notification context, as well as vice-versa, to 
facilitate this mechanism, which increases the size of the notification object from $2^{4}$ to
$2^{5}$ on 32-bit, and $2^{5}$ to $2^{6}$ on 64-bit. 

\section{MCS API}
\label{s:new-api}

We now present the new \gls{MCS} API for \selfour, which alters the core system call API, 
as well as adding new invocations and modifying existing ones. In this section we also
present the semantics of scheduling context donation, which are directly linked to the new API.

\subsection{Waiting system calls}

In \cref{sec:sel4-system-call-and-invocations} we divided the \selfour system call API into two
classes: sending system calls (\code{send}, \code{nbsend}, \code{call}, \code{reply}),
receiving system calls (\code{recv}, \code{nbrecv}), plus atomic combinations of both for fast 
RPC (\code{call}, \code{replyrecv}).  

Our implementation alters the meaning of a receiving system call, and adds a new class of system
call: \emph{waiting} system calls. The difference is simple: receiving system calls are expected to
provide a capability to a resume object to block threads on if a \code{call} is received over
the endpoint being blocked on. Waiting system calls do not, and cannot be paired with a \code{call}
successfully. Only receiving system calls can be used with scheduling context donation, as the reply
object is used to track the call stack.

Additionally, because the TCB reply capability slot is dropped, we remove the \code{reply} system
call as its only purpose was to invoke the capability in the reply slot, which no longer exists.
\code{replyrecv} remains, and invokes the resume capability, sending the reply, before using the
reply in the \code{recv} phase. 

By making the reply channel explicit, we significantly increase the practicality of the IPC
fastpath. In baseline \selfour, any server that served multiple clients and did not reply
immediately---because it needed to block on I/O, for example---would save the reply
capability, moving it from the special TCB slot to a standard slot in the \code{cspace}. Later, when
the server replied to the client, it would invoke the saved reply capability with \send, then block
for new messages with \recv, avoiding the IPC fastpath for \replyrecv. Resume objects avoid the save
fastpath, should the conditions detailed in \cref{sec:sel4-fastpath} be satisified.

\subsection{IPC Forwarding}

For sending system calls, only the system calls that combine a send-phase and a receive-phase can donate
a scheduling context, as a thread must be blocked in order to receive a scheduling context. Both
\call and \replyrecv combine a \send and \recv phase, although with slightly different semantics
with respect to resume objects. However, in both of these cases the \send and \recv are on the same
capability. IPC forwarding allows a \nbsend followed by a \recv on two distinct capabilities, with a
new system call \nbsendrecv. We additionally provide a waiting variant, \nbsendwait, which combines
an \nbsend and \wait. Both variants allow for IPC forwarding, and can donate a scheduling context
along with the IPC, by-passing the call chain. 

The first phase of \nbsendrecv and \nbsendwait must use an \nbsend rather than a \send
to avoid introducing a long-running operation in the kernel in the form of a chain reaction
triggered by a single thread blocking. We demonstrate by example why the send-phase must be
non-blocking.

Consider a set of endpoints, $E_{1}, E_{2},\ldots,E_{n}$ which are all idle, with no messages
pending. If thread $T_{1}$ attempts to \sendrecv,
on ($E_{1},E_{2})$ respectively, the blocking send will block on $E_{1}$. Now if another thread
does the same on ($E_{2},E_{3})$ it will also block, waiting on $E_{2}$. This continues until
$T_{n}$ attempts to \sendrecv on ($E_{n-1},E_{n}$), such that all threads $T_{x}$ are blocked on
endpoints $E_{x}$. Finally, another thread, $T_{z}$ calls receive on $E_{1}$, which triggers $T_{1}$ to then
block on $E_{2}$, allowing $T_{2}$'s send on $E_{2}$ to succeed, causing $T_{2}$ to block on
$E_{3}$ and so on, until each $T_{n}$ is blocked on $E_{n+1}$. This chain reaction is a relatively
unbounded long-running operation, and can only be limited by not providing threads with access to
memory, endpoints, threads and scheduling contexts. To keep the kernel \gls{WCET} small, these
operations are not permitted in \selfour. By making the send-phase non-blocking, the send-phase fails, and
each \gls{TCB} only blocks on the second endpoint, preventing the possibility of a chain-reaction. 

\subsubsection{Yield}

We alter the semantics of \yield: it now causes the head replenishment of the currently running
thread to expire immediately, in addition to placing the thread at the end of the scheduling queue
for its priority. 
For full scheduling contexts, this is the entire timeslice and is compatible with the previous
behaviour of \yield. For partial scheduling contexts, if the next replenishment is not yet ready,
the thread is blocked until it is replenished.

\begin{table}[t]
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{lXll}\toprule
        \emph{System Call} & Class              & Donation? & Definition\\\midrule
        \call                  & sending, receiving & \yes & \cref{api:call}\\
        \send                 & sending            & \no      & \cref{api:send} \\
        \nbsend                & sending            & \no       & \cref{api:nbsend} \\
        \recv                  & waiting            & \yes & \cref{api:recv} \\
        \nbrecv                & waiting            & \yes & \cref{api:nbrecv} \\
        \wait                 & waiting            & \no & \cref{api:wait}\\
        \nbwait                & waiting            & \no & \cref{api:nbwait} \\
        \replyrecv             & sending, receiving & \yes & \cref{api:replyrecv} \\
        \nbsendrecv            & sending, receiving & \yes & \cref{api:nbsendrecv} \\
        \nbsendwait            & sending, waiting   & \no & \cref{api:nbsendwait} \\
        \yield                 & scheduling                & \no & \cref{api:yield} \\
    \end{tabularx}
    \caption{New \selfour system call summary, indicating which system calls can trigger donation.
    and/or receiving messages. }
    \label{t:system-calls}
\end{table}

\cref{t:system-calls} summarises the new MCS system call API, while more detailed 
descriptions of each system call can be found in \cref{label:appendix-api}.

\subsection{Invocations}

Scheduling contexts have 5 invocations, listed in \cref{tab:sched_context_api}. The new control
capability, \code{sched\_control}, has only one invocation \code{configure} for setting the parameters of a
scheduling context (the function definition can be found in \cref{api:schedcontrol_configure}). 
    
\begin{table}[b]
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{>{\texttt\bgroup}l<{\egroup}Xl} \toprule
        \textnormal{\emph{Invocation}} & \emph{Description} & Definition\\\midrule
        bind    & Bind an object (TCB or Notification) to an SC. & \cref{api:schedcontext_bind} \\
        unbind  & Unbind all objects from an SC. & \cref{api:schedcontext_unbind} \\
        unbindObject & Unbind a specific object from an SC. & \cref{api:schedcontext_unbind_object}\\
        consumed & Return the amount of time since the last timeout fault, \code{consumed} or
        \code{yieldTo} was
        called. & \cref{api:schedcontext_consumed}\\ 
        yieldTo  & Place the thread bound to this \gls{SCO} at the front of its priority queue and return
        any time consumed. & \cref{api:schedcontext_yieldto}\\
        \bottomrule
    \end{tabularx}
    \caption{Scheduling context capability invocations.}
    \label{tab:sched_context_api}
\end{table}

Because \selfour only supports sending three capabilities in a single message (including to the
kernel), we add a second method for configuring multiple fields on a TCB in one system call
(\code{setschedparams}) in addition to the existing method (\code{configure}), as full configuration
of a TCB requires up to six capabilities to be set. Timeout handlers are left as a setter only, as 
this is a less common operation. In total, we alter two methods on TCB objects
and add three new methods, shown in \cref{tab:tcb_api}. 

\begin{table}
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{>{\texttt\bgroup}l<{\egroup}Xll} \toprule
        \textnormal{\emph{Invocation}} & \emph{Description} & New? & Definition \\\midrule
        configure & Set the cnode, vspace, and \gls{IPC} buffer. & \no & \cref{api:tcb_configure}\\
        setmcpriority & Set the \gls{MCP}. & \yes & \cref{api:tcb_setmcpriority} \\
        setpriority & Set the priority. & \no & \cref{api:tcb_setpriority} \\
        setschedparams & Set \gls{SCO}, \gls{MCP}, priority and fault endpoint. & \yes &
        \cref{api:tcb_setschedparams} \\
        settimeoutep & Set the timeout endpoint. & \yes & \cref{api:tcb_settimeoutep} \\
        \bottomrule
    \end{tabularx}
    \caption{New and altered \gls{TCB} capability invocations.}
    \label{tab:tcb_api}
\end{table}

\subsection{Scheduling context donation}

Conceptually there are three types of scheduling context donation: lending, returning, and
forwarding a scheduling context. The type of donation that occurs, and if it can occur, depends on
the system call used. 

Lending a scheduling context occurs between caller and callee on a \call, and
the semantics are simple: if a thread 
blocked on an endpoint does not have a scheduling context, the scheduling context is transferred
from sender (caller) to receiver (the callee), and the resume object is pushed
onto the call-stack, such that the \gls{SCO} can be returned. If the caller already has a scheduling
context, scheduling context donation does not occur. Note that lending also works for
handling faults, so thread fault handlers can be passive.

Lent scheduling contexts are returned, either when the resume object is invoked. Resume objects 
can be invoked directly by the send-phase of a system call, or indirectly, by using the
resume object in the receive phase of a system call, thereby clearing it for another client to block
on, or deleting the resume object.
No matter the state of the callee the \gls{SCO} is returned, which if done
incorrectly (via a \send), can leave the callee runnable but not blocked on an endpoint, such that 
the callee cannot receive further scheduling contexts. Correct usage by a passive thread is with \replyrecv,
\nbsendrecv, blocking the passive thread such that it is ready to receive another scheduling
context. Like lending, scheduling contexts can also be returned by a fault handler replying to a fault message.

Forwarding a scheduling context does not establish a call-chain relationship, and allows
for scheduling contexts to be transferred between threads in non-RPC patterns. The semantics are
as follows: if a \nbsendrecv is used, and the receiver does not have a scheduling context,
the scheduling context is forwarded. 

Scheduling contexts can only be donated and received on specific system calls, indicated in
\cref{t:system-calls}. 

\subsubsection{Passive servers}

Forwarding is required to initialise passive servers, as passive servers must first initialise any 
server state before blocking and being rendered passive. Example user-level code for this process is 
provided in \cref{list:passive-server}. 

\begin{listing}[h]
\begin{ccode}
    void initialiser(seL4_CPtr server_tcb, seL4_CPtr init_sc, seL4_CPtr init_endpoint) 
{
    /* start server */
    seL4_TCB_Resume(server_tcb);
    /* wait for the server to tell us it is initialised */
    seL4_Wait(init_endpoint, NULL);
    /* convert the server to passive */
    seL4_SchedContext_Unbind(init_sc);
}

void passive_server(seL4_CPtr init_endpoint, seL4_CPtr endpoint, seL4_CPtr reply) 
{    
    seL4_MessageInfo_t info = init_server_state();
    seL4_Word badge;

    /* signal to the initialiser that this server is ready 
       to be converted to passive, and block on the endpoint 
       with resume object reply */
    info = nbsendrecv(init_endpoint, info, endpoint, &badge, reply);
    while (true) {
        /* when the server wakes, it is running on a
           client scheduling context */
        info = process_request(sender);
        /* reply to the client and block on endpoint, with resume 
           object reply */
        seL4_ReplyRecv(endpoint, info, &badge, reply);
    }
}

void client(seL4_CPtr endpoint, seL4_MessageInfo_t message) {
    /* send a message to the passive server */
    seL4_Call(endpoint, message);
}
\end{ccode}
\caption{Example initialiser, passive server, and client.}
\label{list:passive-server}
\end{listing}

\subsubsection{Notification Binding}

Passive servers can receive scheduling contexts from their bound notification object, allowing for
the construction of single-threaded servers which receive both notifications and IPC messages. 
The semantics are as follows: if a TCB receives a notification from its bound notification
object, and that TCB does not have a scheduling context, the TCB receives the scheduling context.
If a TCB blocks on an endpoint, and is running on its bound notification object, the TCB is
rendered passive again. 

\section{Data Structures and Algorithms}

\TODO{fix intro to this section}
Now we discuss changes to the data structures and algorithms in the kernel 
in order to implement the mechanisms.  kernel scheduler and how the new \glspl{SCO} interact which the
scheduling algorithm to provide temporal isolation and asymmetric protection.


We make changes to the scheduler and structure of the scheduler to support temporal isolation: we
convert the scheduler to tickless, add a release queue of threads waiting for budget replenishment
and introduce the invariant that threads without scheduling contexts are not eligible for
scheduling.

\subsection{Accounting}

All processing time is accounted to the scheduling context of the currently running \gls{TCB}. 
In this section we first establish \emph{how} that time is accounted, and then \emph{when} it is
accounted.

There are two ways to account for time in an \gls{OS} kernel:
\begin{itemize}
    \item in fixed time quanta, referred to as \emph{ticks},
    \item in actual time passed between events, referred to as \emph{tickless}.
\end{itemize}

Using ticks, timer interrupts are set for a periodic tick and are
handled even if no kernel operation is required, incurring a preemption overhead.
This approach has the advantage of simplicity: the timer implementation in the kernel is 
stateless and the timer driver can be set once, periodically, never requiring reprogramming. 
In older-generation hardware, reprogramming the timer device incurred a significant cost, enough to
ameliorate the preemption overhead. However, this design is not without limitations:
the precision of the scheduler is reduced to the length of the tick. Precision must be traded for
preemption overhead: reducing the tick length increases precision, but also increases 
preemption overhead. Preemption overhead is particularly problematic for real-time systems, as
the \gls{WCET} each real-time task is inflated by the \gls{WCET} of the kernel for every preemption.

Tickless kernels remove this trade-off by setting timer interrupts for the exact time of the next
event. As we show in \cref{s:eval-timer}, the cost of reprogramming the timer device on modern
hardware is minimal, making the tickless design feasible. Consequently, we convert \selfour
to a tickless, a non-trivial change due to the fact that the kernel is non-preemptible (save for
explicit preemption points in a few long-running operations). 

\cref{figure:tickless} illustrates the new control flow of the tickless kernel, and shows
how and when processing time is charged to scheduling contexts.

We alter the IPC fastpath conditions introduced in \cref{sec:sel4-fastpath} to include one
more condition, which allows us to avoid accessing the timer device on the fastpath:
\begin{itemize}
    \item The receiver must be passive. For \call, this means the caller must be passive, and
          for \replyrecv, the TCB blocked on the resume object must be passive.
\end{itemize}

On kernel entries other than the IPC fastpath, the kernel updates the current
timestamp and stores the time since the last entry. This is required as when preemption occurs, the
preempting thread is charged for the time since the kernel entry. Without knowing if the kernel entry will
trigger a thread switch in advance, the kernel must record the time for each entry. However, if the
recorded timestamp is not acted upon, the time is rolled back to the previously recorded value,
this avoiding reprogramming the timer unnecessarily.

We add a new invariant over the kernel scheduler: any thread in the 
scheduling queues must have enough budget to exit the kernel. The budget required is twice the
kernel's WCET, enough to get into the kernel and block, and enough to get out of the kernel 
when woken. 
Consequently the scheduler precision becomes twice the kernel's WCET.

This invariant is required as it simplifies the kernel design and actually minimises the WCET: when
a
thread runs out of time it may need to raise a timeout exception resulting in delivering an IPC to a
timeout handler. By requiring that anything in the scheduler queue, or any endpoint queue, must have
enough budget to wake up we avoid needing to potentially raise timeout exceptions on many wakeup
paths in the kernel.

If the available budget is insufficient, the kernel pretends the timer has already fired,
resets the budget and adds the thread to the release queue. If the entry was due to a system call,
the thread will retry that call once it wakes with further budget.
Once the thread is awoken it will retry the system call.

Threads are only charged when the scheduling context changes or expires, in order to avoid
reprogramming the timer which can be expensive on many platforms. 
If there is no \gls{SCO} change, the timestamp update is rolled back by subtracting the
stored consumed value from the timestamp.

\begin{figure}
    \centering 
    \includegraphics{tickless}
    \caption{Kernel structure}
    \label{figure:tickless}
\end{figure}

\subsubsection{Domain scheduler}

\TODO{update}

While the real-time amendments to the scheduler are compatible with the domain scheduler, either the domain scheduler or the real-time amendments should be used for real-time scheduling.
This is because domains are non-preemptive and as a result can only be used for non-preemptive real-time scheduling, where domain parameters exactly match real-time scheduling parameters.
Using more than one domain when preemptive real-time scheduling is will result in missed deadlines.


\subsection{Enforcement}

% timeout exceptions, release queue, sporadic servers
The main change required to the existing scheduler is the addition of a \emph{release queue} per
processing core. If a
preempted thread does not have any available replenishments, the kernel removes the thread from the
ready queue to the release queue, retaining the invariant for the ready queue, which the release
queue is characterised as holding all threads that would be runnable but are presently out of
budget. The queue is ordered by the timer when the next replenishment is available.

\subsubsection{Priority queues}
\TODO{for replenishment queues and IPC}
Priority queues for both scheduling algorithms are implemented as ordered lists, with $O(n)$ insertion and removal complexity.
The most frequent operation on the lists is to remove the head, which is $O(1)$.
We choose a list over a heap for increased performance and reduced verification burden.

A list-based priority queue out performs a heap-based priority queue for small $n$ in our implementation up to around $n = 100$.
This $n$ is larger than one would expect in a traditional \gls{OS}, where heap implementations are array-based in contiguous memory with layouts optimised for cache usage.
However, in order to provide isolation and confidentiality, \selfour kernel memory is managed at user-level, as discussed in %TODO{section}.
Consequently, to put a \selfour kernel object into a heap, the pointers for the heap implementation must be contained within the object, which could be anywhere in memory as chosen by the user.
This means that \selfour heap implementations are non-contiguous, and must be pointer based, resulting in a much larger cache footprint with worse performance than an array-based approach.

verification.
Of course, even if the heap implementation is slower, given a sufficient number of tasks a heap will scale better than a list.
However, we do not expect systems to run a large amount of real-time tasks, as \selfour target applications generally run virtualised Linux along-side a few critical real-time tasks.
Additionally, the reduced complexity of a list compared to a heap will result in faster verification, so consequently a list implementation looks favourable.

\subsubsection{Timeout Exception Handlers}

\subsection{Sporadic Servers}
\label{sec:impl-sporadic}

Scheduling contexts contain a circular buffer for sporadic task replenishments.
Each replenishment has an amount of time that stands to be replenished, and the absolute time from when that replenishment can be used, as shown in \cref{tab:refill}.
When \texttt{seL4\_SchedControl\_Configure} is called on an inactive scheduling context, the amount is set to the budget and the replenishment time to the current time.
At all times, the sum of the amounts in the replenishment buffer is equal to the configured budget.
The maximum size of this buffer is statically configurable, and the minimum size is one.
Users can specify the amount of extra refills a scheduling context can have, up to the static maximum.
This extends the approach used by Quest-V~\citep{Danish_LW_11}, which has as static limit of 32
replenishments % TODO expand, specify IO or find elsewhere. 


Scheduling contexts with zero extra refills behave like polling servers (\cref{p:polling-servers}), otherwise they behave as sporadic servers (\cref{p:sporadic}), allowing application developers to tune the behaviour of threads depending on their preemption levels and execution durations.

The algorithms to manage replenishments are taken from \citet{Danish_LW_11}, with adjustments to support periods of 0 (for round robin threads) and to implement a minimum budget.
Whenever the current scheduling context is changed, \texttt{check\_budget} as shown in Listing \ref{list:check-budget} is called to bill the amount of time consumed since the last scheduling context change.
`check\_budget`
If the budget is not completely consumed by \texttt{check\_budget}, \texttt{split\_budget} as shown in Listing \ref{list:split-check} is called to schedule the subseqeunt refill for the chunk of time just consumed.
If the replenishment buffer is full, or the amount consumed is less than the minimum budget, the amount used is merged into the next replenishment.
The scheduling context being switched to has \texttt{unblock\_check} Listing (\ref{list:unblock-check}) called on it, which merges an replenishments that are already available, avoiding unneccessary preemptions.

When \texttt{seL4\_SchedControl\_Configure} is called on an active scheduling context, the refills are adjusted to reflect the new budget and period but respect the sliding window constraint.

Round-robin threads have full reservations: $T = C$, entitling them to 100\% of the processor but subject to preemption every time $C$ is used.
Clearly we don't want to track replenishments for round-robin threads, so the kernel detects if round robin scheduling contexts when \texttt{seL4\_SchedControl\_Configure} is called, and sets the period to 0.
This avoids much special casing in the replenishment code, as round-robin threads are always ready (we always add 0 to the replenishment time).

\TODO{Say what head refill does}
\begin{listing}[b]
\begin{ccode}

uint_64_t check_budget(sched_context_t *sc, uint64_t usage) {
  while (head_refill(sc).amount <= usage) {
    // exhaust and schedule replenishment
    old_head = pop_head(sc);
    usage -= old_head.amount;
    old_head.time += sc->period;
    add_tail(sc, old_head)
  }

  /* handle budget overrun */
  if (usage > 0 && sc->period > 0) {
    // delay refill by overrun
    head_refill(sc).time += usage;
    // merge replenishments if time overlaps
    if (refill_size(sc) > 1 &&
        head_refill(sc).time + head_refill(sc).amount
        >= refill_next(sc).time) {

      refill_t old_head = pop_head(sc);
      head_refill(sc).amount += old_head.amount;
    }
  }
}
\end{ccode}
\caption{Check budget routine.}
\label{list:check-budget}
\end{listing}

\begin{listing}
    \begin{ccode}
void split_check(sched_context_t *sc, uint64_t usage) {
  uint64_t remnant = head_refill(sc).amount - usage;
  if (remnant < MIN_BUDGET && refill_size(sc) == 1) {
    // delay entire replenishment
    // refill too small to use and nothing to merge with */
    head_refill(sc).time += sc->period;
    return;
  }

  if (refill_size(sc) == sc->refill_max || remnant < MIN_BUDGET) {
    // merge remnant - out of space or its too small
    pop_head(sc);
    head_refill(sc).amount += remant;
  } else {
    // split the head refill
    head_refill(sc).amount = remnant;
  }

  // schedule the used amount
  refill_t split;
  split.amount = usage;
  split.time = head_refill(sc).time + sc->period;
  add_tail(sc, split);
}
\end{ccode}
\caption{Split check routine.}
\label{list:split-check}
\end{listing}

\begin{listing}
    \begin{ccode}
void unblock_check(sched_context_t *sc) {
  if (!head_refill(sc).time > now)) {
    return;
  }

  head_refill(sc).time = now;
  // merge available replenishments
  while (refill_size(sc) > 1) {
    if (refill_next(sc).time < now + head_refill(sc).amount) {
      refill_t old_head = pop_head(sc);
      head_refill(sc).amount += old_head.amount;
      head_refill(sc).time = now;
    } else {
      break;
    }

    if (head_refill(sc).amount < MIN_BUDGET) {
      // second part of split_check can leave refills
      // with less than MIN_BUDGET amount.
      // detect them here and merge.
      refill_t old_head = pop_head(sc);
      head_refill(sc).amount += old_head.amount;
    }
}
\end{ccode}
\caption{Unblock check routine.}
\label{list:unblock-check}
\end{listing}


\TODO{FINISH THIS CHAPTER -- everything beyond here is stale}
% Yield behaviour







\subsubsection{Real-time threads}

The scheduling of real-time threads is more complicated than that of rate-limited or best-effort
threads, as the concept of a sporadic job, including job release time and job completion, need to be
supported by the kernel API.

We define the initial job release as when a thread is resumed: the available budget is set to the
total budget in the reservation for that thread.

Job completion is more complicated.  As described in section \Cref{sec:sporadic-task-model}, job
completion occurs when a job blocks waiting for the next job to be released.  Any budget left by the
current job is released as slack into the system, which means the available reservation budget drops
to 0.  If a job is time-triggered, such that it only relies on time for job release, then next job
will be released once the period has passed.  If a job is event-triggered, then the next job should
be released once the period has passed \textit{and} an external event occurs, such as an interrupt.

Simply defining job completion as when a task blocks is not sufficient as jobs can also block for
other reasons, like polling I/O, or waiting for an asynchronous server.  Unintentionally completing
a job by blocking is incorrect, as it would result in real-time threads receiving less processing
time than they have reserved.

One design option we considered was to use the yield system call to complete the current job.
However this approach would not enable a thread to receive a notification on job release, requiring
more system calls to retrieve notifications.

\begin{listing}
\begin{ccode}
    for (;;) {
        // job is released
        doJob();
        // job completes before deadline or is postponed by CBS
        // sleep until next job is ready
        seL4_Word badge;
        seL4_Wait(bound_async_endpoint, &badge);
    }
\end{ccode}
\caption{Example of a basic sporadic real-time task on sel4}
\label{list:sporadic-sel4}
\end{listing}



\subsubsection{Exception}

Temporal faults allow the user to implement custom handling for budget expiry.
A temporal fault endpoint for budget expiry is set per thread, so a server can set up a dedicated thread to wait for temporal faults.
The dedicated thread must be assigned its own reservation for handling faults.
The thread can be used to complete the request (as an emergency reservation), to rollback the request and return an error from the server, or for other purposes.
Of course, servers then must be thread safe such that the fault handling thread does not corrupt state.



\section{Summary}

This section has outlined the details of integrating resource kernel concepts of enforcement, admission and scheduling into \selfour.
We also outlined our approach to resource sharing, using scheduling context donation over IPC, and showed how this supports servers using reservation-per-thread, migrating-threads, temporal exceptions or bandwidth inheritance.


