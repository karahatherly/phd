\chapter{Design \& Model}
\label{chap:model}

We now present our design and model for mixed-criticality scheduling support in a high-assurance
system such as \selfour. 

Our goal is to provide support in the kernel for mixed-criticality workloads.  This involves
supporting tasks of different real-time strictness (\gls{HRT}, \gls{SRT}, best effort), different
criticalities, and different levels of security.  Such tasks should not be forced into total
isolation, but be permitted to share resources without violating their temporal correctness
properties through mechanisms provided by the kernel.

To achieve this we require temporal isolation: a feature of resource kernels, whose mechanisms we
apply to our research platform, \selfour.  However temporal isolation is not enough: mixed-criticality
systems require asymmetric protection rather than temporal isolation, as a result we leverage
traditional resource kernel reservations but decouple them from priority, allowing the processor to
be overcommitted while providing guarantees for the highest priority tasks.

In this section we first address how we integrate resource kernel mechanisms with \selfour with
mechanisms for temporal isolation. We then explain
priority decoupling and our model for temporal isolation in shared resources, with clients of
different criticalities, temporal sensitivity, and trust.

Our design goals are as follows:
\begin{description}\sloppy
    \item[Capability-controlled enforcement of time limits:] In general, capabilities
      help to reason about access rights. Furthermore, they allow a
        seamless integration with the existing capability-based spatial
          access control of security-oriented systems such as \selfour.
    \item[Policy freedom:] In line with microkernel philosophy
        \citep{Heiser_Elphinstone_16}, the model should not force systems
        into a particular resource-management policy. In particular, it
        should support a wide range of scheduling policies and
        resource-sharing models, such as locking protocols.
%    \item[Explicit notion of criticality] that is separate from priority /
 %                   time sensitivity.
    \item[Efficient:] The model should add minimal overhead over the best
                      existing implementations. In particular, it should be compatible
                        with fast IPC implementations in high-performance microkernels.

    \item[Temporal isolation:] The model must allow system designers to create systems where a
        temporal failure in one component cannot cause a temporal failure in another part of the
        system, even in the case of shared resources.

    \item[Overcommitment:] The model must allow systems to be specified that are not necessarily
        schedulable: as the degree and nature of overcommitment is a core policy of a particular
        system. Overcommitment is also key to providing  asymmetric protection, where high
        criticality tasks can disrupt low criticality tasks.

    \item[Safe resource sharing:] mechanisms for sharing between applications of time sensitivity
        and criticality.
\end{description}

The model  provides temporal isolation mechanisms from the kernel, while allowing for more complex
scheduling policies to be implemented at user level.

\section{Scheduling}

We outlined four core resource kernel mechanisms---admission, scheduling, enforcement and
accounting---that are essential to resource kernels for implementing temporal isolation.  However,
as noted in \Cref{sec:resource-kernels}, such kernels are monolithic, where all policy, drivers and
mechanisms are provided by the kernel.

Microkernels like \selfour offer a different design philosophy, based on the principle of
minimality~\citep{Liedtke_95}, where mechanisms are only included in the kernel if they would
otherwise prevent the implementation of a systems required functionality.  Existing literature is
divided as to whether resource kernel mechanisms should be provided by the kernel or at user-level.

The goals of resource kernels do not directly align with that of microkernels in general.  This is
because microkernels do not directly manage all resources in the system, but provide mechanisms for
the system designer to implement custom resource management policies.  

In \selfour, mechanisms for physical memory, device memory, interrupt and \IO port management are
exposed to the user via the capability system, as outlined in \cref{chap:sel4}. As a result, the
only resource that the kernel needs to provide reservations for is processing time.  We now examine
how each of the four resource kernel mechanisms are implemented in our model.

\subsection{Scheduling}

There are two design choices relevant to scheduling:

\begin{itemize} 
    \item Should a scheduler be provided in the kernel at all? 
    \item Should the scheduling algorithm be fixed (\gls{FP}) or dynamic (\gls{EDF}) priority?
\end{itemize}

\subsubsection{Kernel scheduling}

We retain the scheduler in the kernel, unlike \composite, for two reasons: to maintain a small
trusted computing base, and for performance. Any system with multiple threads of execution, which is
required in a mixed-criticality system, must have a scheduler, which for the purposes of temporal isolation 
is part of the trusted computing base. If a system must have a scheduler, and in a mixed-criticality
system, with untrusted components, that scheduler must be in a separate protection domain to those
untrusted components, then a scheduling decision will require at least two context switches: from
the preempted thread to the scheduler, and from the scheduler to the chosen thread. Placing a basic
scheduler in the kernel reduces this overhead. 

Additionally, as the scheduler is a core component of the system it must be verified: by keeping the
scheduler in the kernel we maintain the current verification. Verification of a user level scheduler
and its interaction with the kernel is a far more complex task, especially as verification of
concurrent systems is very much an open research challenge. 

One might claim maintaining a scheduler in the kernel is a violation of our goal of policy freedom.
However we maintain this is not the case, which comes down to our choice of fixed priorities over
\gls{EDF}.

\subsubsection{Fixed priorities}

Our model uses \gls{FP} scheduling as a core part of the kernel, with the addition of mechanisms
that allow for the efficient implementation of user-level schedulers.
We choose \gls{FP} over \gls{EDF} for three reasons; 
fixed-priority is dominant in industry
as shown in \cref{chap:operating-systems} and maps well to \gls{FP} with \gls{RM} priority assignment; and dynamic scheduling policies like
\gls{EDF} can be implemented at user level; and \gls{FP} has defined behaviour on overcommitted
systems.

\gls{EDF} scheduling can be implemented by using a single priority for EDF's dynamic priorities, as we
will demonstrate in \cref{s:edf-impl}.
However the opposite is not true: mapping the dynamic priorities of EDF to a fixed-priority
is non-trivial and would come with high overheads. 
Our approach is consistent with existing designs in Linux  and ADA~\citep{Burns_Wellings:crtpa}, which
support both scheduling algorithms, usually with \gls{EDF} at a specific priority.

We do not consider Pfair scheduling (recall \cref{s:pfair}) an option, as its high interrupt
overheads and fairness properties are not suitable for hard real-time systems.  Again however, it is
possible to implement a Pfair scheduler at user level.

The final reason to base the approach on fixed priorities is the ability
to reason about the behaviour of an
overcommitted system. Overcommitting, is important for achieving high
actual system utilisation, given the large time buffers required by
critical hard real-time threads. It is also core to keeping the kernel
policy-free: The degree and nature of overcommitment is a core policy
of a particular system. For example, the policy might require that the
total utilisation of all \crit{high} threads is below the RMS
schedulability limit of 69\%, while \crit{low} threads can overcommit,
and the degree of overcommitment may depend on the mix of hard RT,
soft RT and best-effort threads. Such policy should be defined and
implemented at user level rather than in the kernel.

As discussed in \autoref{s:overload}, the result of overload in an
EDF-based system is hard to predict, and such a system is hard to
analyse. In contrast, with fixed priority the result is easy to
understand: If the sum of utilisations of threads at priority \(\geq
P\) is below the utilisation bound, then all those threads will meet
their deadlines, while any thread with priority \(<P\) may miss. This
allows easy analysis of schedulability, including when the system
criticality changes.

\subsection{Scheduling contexts}
\label{s:scs}

At the core of the model is the \emph{\gls{SC}} as the
fundamental abstraction for time allocation, and the basis of 
the enforcement and accounting mechanisms in our model.
A \gls{SC} is a representation
of a reservation in the object-capability system, which means that 
they are first-class objects, like threads, address spaces, or
communication endpoints (ports). An SC is represented by a capability to a
scheduling context object (scCap).

In order to run, a thread needs an scCap, which represents the
maximum CPU bandwidth the thread can consume.
In a multicore system, an SC represents the right to access a
particular core. Core migration, e.g.\ for load balancing, is policy
that should not be imposed by the kernel but implemented at user
level. A thread is migrated by replacing its SC with one tied to a
different core. This renders the kernel scheduler a partitioned scheduler, 
which aligns with our efficiency goal; partitioned schedulers outperform global
schedulers~\citep{Brandenburg:phd}.

The unfungible nature of time in real-time systems requires that the
bandwidth limit must be enforced within a certain time window. We
achieve this by representing an SC by a \emph{period}, \(T\), and a
\emph{budget}, \(C\), where \(C\leq T\) is the maximum amount of time
the SC allows to be consumed in the period. \(U=\frac{C}{T}\) represents the
maximum \emph{CPU utilisation} the SC allows. The SC can be viewed as
a generalisation of the concept of a time slice that is used on many
systems (including present mainline \selfour).

In order to support MCS, we do not
change the meaning of priority, but what it means for a thread to be
\emph{runnable}: We associate each thread with a SC, and
make it non-runnable if it has exhausted its budget. 


\glspl{SC} can be gracefully integrated into the 
existing model used in \selfour,
logically replacing the time slice attribute with the scCap. 

Unlike Fiasco's scheduling contexts~\citep{Lackorzynski_WVH_12}, which
are superficially similar to ours, we retain priorities as a thread
attribute rather than making them part of SCs.\footnote{Fiasco's scheduling
contexts were developed for an old API version that was not
capability-based, and it is not obvious how they would integrate
with the capability system of present Fiasco.OC.} The advantage of keeping the two
orthogonal will become evident in \cref{s:locking}.

\subsection{Accounting}

We introduce a new invariant that the currently running thread must have a scheduling context 
with available budget, as all time consumed is billed to the current scheduling context.
Any time executed on a processor is billed to the current threads scheduling
context. We specifically do not cater for dynamic frequency scaling and ensure that it is turned off
during testing --- this is out of scope and a topic for future work. 

\subsection{Enforcement}

Our model offers a single mechanism for enforcement: threads can have an optional timeout fault
endpoint.
If a timeout fault endpoint is provided, a fault message is sent to that endpoint should that thread
exhaust its budget, allowing the system to enact an enforcement policy.
Threads without a timeout fault endpoint are not schedulable until their budget is replenished.

The timeout fault
endpoint is separate from the existing thread fault endpoint, as the semantics are different. For the
existing fault handler threads block if a fault occurs if no fault handling endpoint exists. In the
case of a timeout fault handler, the thread remains runnable and is only blocked if a fault message
is delivered to the timeout fault endpoint. Otherwise, the kernel will not block the thread but
as it can run again once its budget is replenished.

Timeout fault handling threads must have their own \gls{SC} to run on, with enough budget to handle
the fault.


\subsection{Admission control}
\label{sec:model-admission}

Setting budgets is admission control and requires appropriate
privilege. The total available time on a core is represented in a
(virtual) per-core scheduling-control capability, \code{sched\_control}.
Access to this capability provides the admission-control
privilege. This is analogous to how \selfour controls the right
to receive interrupts, which is controlled by the IRQ\_control
capability as introduced in \cref{s:capabilities}. Like time, IRQ sources are non-fungible.

Unlike the reservations in resource kernels, the scheduling context does not 
act as a lower-bound on CPU bandwidth that a thread can consume. This, combined with the
user-level admission control, is also key to allowing system designers to over-commit the system. 

By designating admission tests as user-level policy, we allow system designers complete freedom
in determining which admission test to use, if at all, and when that test should be done.
Consequently, they can be run dynamically at run-time, or offline, as per user-level policy.

Thus, the kernel places no restriction on the creation of reservations apart from minor validity
checks (\ie $C \leq T$).
For example, some high-assurance systems may sacrifice utilisation for safety with a very basic but easily verifiable, online, admission test.
Other implementations may conduct complex admission tests offline in order to obtain the highest possible utilisation, using algorithms that are not feasible at run time.
Some systems may require dynamic admission tests that sacrifice utilisation or have increased risk.
Basic systems may require a simple break up of the processing time into rate-limited reservations.
By taking the admission test out of the kernel, all of these extremes (and hybrids of) are optional policy for the user.

A consequence of this design is that more reservations can be made than processing time available.
This is a desirable feature: it allows system designers to overcommit the system, while features of the scheduling mechanisms provided by the kernel guarantee that the most important tasks get their allocations, if the priorities of the system are set correctly.

However, allowing any thread in the system to create reservations could result in overload behaviour and violation of temporal isolation.
To prevent this, admission control is currently restricted to the task that holds the scheduling
control capability for each processing core.

\subsection{Replenishment}

Scheduling contexts can be \emph{full} or \emph{partial}. A thread with a \emph{full} SC, where
\(C=T\), may monopolise whatever
CPU bandwidth is left over from higher-priority threads, while high-priority threads with full
\glspl{SC} may monopolise the entire system. 
Partial \glspl{SC}, where \(C<T\), are not runnable once
they have used up their budget, until it is replenished, and form our mechanism for temporal
isolation.  

Full \glspl{SC} are key to maintaining policy freedom and performance; while systems
must be able to use our mechanisms to enforce upper bounds on execution, the usage of 
those mechanisms is policy. If a task is truly trusted, no enforcement is required,
as in standard \gls{HRT} systems. They can also be used for best-effort threads which run in slack time.
The \(C\) of a full \gls{SC} represents the timeslice, which once expired results in round robin
scheduling at that priority. 

From a performance perspective full \glspl{SC} prevent mandatory preemption overheads 
that derive from forcing all threads in a system to have a reservation. Threads with a full
budget incur no inherent overhead other than the preemption rate $1/T$.

Threads with \emph{partial} \glspl{SC} have a limited budget, which forms an upper
bound on their execution. For replenishment we
use the sporadic servers model as described in \cref{p:sporadic} with an
implementation based on the algorithms presented
by~\citet{Stanovic_BWH_10}. 

The combination of full and partial \glspl{SC}  and the ability to overcommit distinguishes our
model from that of resource kernels. However, a full resource kernel can be 
constructed on top of the mechanisms our model provides.
\TODO{describe how to implement this}

\subsubsection{Sporadic servers}
\label{s:sporadic}
We use sporadic servers as they provide a mechanism for isolation without requiring the kernel
to have access to all threads in the system, unlike other approaches discussed in
\cref{background:fp-isolation}, \eg  priority exchange servers and slack stealing.
Deferrable servers do not require global state but are avoided due to the back-to-back
problem. 

Recall that sporadic servers work by preserving the
\emph{sliding window} constraint, meaning that during any time
interval not exceeding the period, no more than $C$ can be consumed.
This stops a thread from saving budget until near the end of its
period, and then running uninterrupted for more than $C$. It is achieved by tracking any leftover
budget when
a thread is preempted at time \(t\), and scheduling a replenishment for time  $t+T$.

In practice we cannot track an infinite number of
replenishments, so in a real implementation, once the number of
queued replenishments exceeds a threshold, any excess budget is
discarded. If the threshold is one, the behaviour degrades to polling
servers~\citep{Sprunt_SL_89a} where any unused budget is lost and the
thread cannot run until the start of the next period.

There is an obvious cost to replenishment fragmentation that will
arise from preemptions, and polling servers are more efficient in the
case of frequent preemption \citep{Li_WCM_14}; an arbitrarily high
threshold makes little sense. The optimal value depends on
implementation details of the system, as well as the characteristics
of the underlying hardware.
We therefore make the threshold an attribute of the SC. \glspl{SC} are variably sized,
such that system designers can set this bound per SC. This is a generalisation of the approach used
in Quest-V~\citep{Danish_LW_11}, where \IO reservations are polling servers and other reservations 
are sporadic servers, as policy which can easily be implemented at user-level with variably sized
\glspl{SC}.

\subsection{Budget overrun}\label{s:timeout}

Threads may exhaust their budgets for different reasons. A budget may
be used to rate-limit a best-effort thread, in which case budget
overrun is not different to normal time-slice preemption of
best-effort systems. A budget can be used to force an untrusted thread
to adhere to its declared WCET. Such an overrun is a contract violation, which may be reason
to suspend the thread or restart its subsystem. Finally, an overrun by
a critical thread can indicate an emergency situation; for example,
critical threads may be scheduled with an optimistic budget to provide
better service to less critical threads, and overrun may require
provision of an emergency budget or a system-criticality change.

Clearly, the handling of overrun is a system-specific policy, and the
kernel should only provide appropriate
mechanisms for implementing the desired policy. Our core mechanism is
the \emph{timeout exception}, which is raised when a thread is
preempted. To allow the system to handle the exceptions, each thread
is optionally associated with a timeout exception handler, which is
the temporal equivalent to a (spatial) protection exception. When a
thread is preempted, the kernel notifies its handler via an IPC message. The
exception is ignored when the thread has no timeout exception handler, and the thread
can continue to run once its budget is replenished.

Similar to a page fault handler, timeout fault handlers can be used to adjust thread parameters dynamically as may be required for a \gls{SRT} system, or raise an error.
The handler has a choice of a range of overrun policies, including:
\begin{itemize}
      \item providing a one-off (emergency) budget to the thread and letting it continue,
       \item permanently increasing the budget in the thread's SC,
       \item killing/suspending the thread, and
       \item changing the system criticality level.
       \end{itemize}
Obviously, these are all subject to the handler having sufficient
authority (\eg \code{sched\_control}.

If a replenishment is ready at the time the budget expires, the thread
is immediately runnable. It is inserted at the end of the ready queue
for its priority, meaning that within a priority, scheduling of
runnable threads is round-robin.

\subsection{Priority assignment}

Assignment of priorities to threads is user-level policy. One approach is to simply use
rate-monotonic scheduling: where priorities are assigned to threads based on their period, and
threads use scheduling contexts that match their sporadic task parameters.  Each thread in the
system will be temporally isolated as the kernel will not permit it to exceed the processing time
reservation that the scheduling context represents.

However, the system we have designed offers far more options than simple rate-monotonic fixed-priority
scheduling.  Policy freedom is retained as reservations simply grant a potential right to processing
time, at a particular priority.  What reservations actually represent is an upper bound on
processing time for a particular thread.  Low priority threads are \emph{not} guaranteed to run if
reservations at higher priorities use all available CPU.  However, threads with reservations at low
priorities will run in the system slack time, which occurs when threads do not use their entire
reservation.

The implication is that a system could use a high range of priorities for rate-monotonic threads,
while best-effort and rate-limited threads run at lower priorities.  Another alternative is to have
real-time threads running at the EDF-priority, where they will be temporally contained, with
non-real time threads running at lower priorities.  Many other combinations are possible.

\subsection{Asymmetric Protection}

Recall that in a mixed-criticality system, asymmetric protection means that tasks with higher
criticality can cause deadline misses in lower criticality tasks.  Two approaches to mixed
criticality scheduling that provide asymmetric are slack scheduling and
\gls{RTA}\citet{Burns_Davis_14}.

Under slack scheduling, low-criticality tasks run in slack time of high criticality tasks.  Our
model supports this easily: high criticality tasks are given reservations to all of processing time at
high priorities, and low criticality tasks given reservations at a lower priority band.

\gls{RTA} relies on suspending low criticality tasks if a high criticality task runs for longer than
expected.  In general, \gls{RTA} schemes involve two system modes: a \crit{HI} mode and a \crit{LO}
mode.  In \crit{LO} mode, high criticality tasks run with smaller reservations, and the remaining
CPU time is used for low criticality tasks.  If a high criticality task does not complete before its
\crit{LO} mode reservation is exhausted, the system switches to \crit{HI} mode: all low criticality
tasks are suspended.  This is also supported by our model: a high priority scheduling thread can be
set up to receive temporal faults when a task does not complete before its budget expires.  On a
temporal fault, the scheduling thread can extend the reservation of the high priority task and
suspend all low priority tasks.

\subsection{Summary}

The scheduling, accounting and enforcement mechanisms presented are sufficient to support temporally
isolated, fixed-priority or \gls{EDF} scheduled real-time threads. By keeping admission control
out of the kernel we preserve policy freedom, while the mechanisms provided allow for a full
resource kernel to be built, or other types of system which require the ability to over-commit.
Additionally, we have maintained
support for best-effort threads, and have added the ability to provide asymmetric protection for
mixed-criticality workloads. 
In the next section, we show how our model provides mechanisms for resource sharing. 

\section{Resource sharing}\label{s:locking}

Bringing mixed-criticality to resource sharing is important for future cyber-physical systems. We 
present our model for resource sharing in terms of the three core mechanisms taken from resource
kernels---prioritisation, charging and enforcement---presented in \cref{sec:resource-kernels}.

We consider shared resources as separate threads accessed via synchronous \gls{IPC}, as this
mechanism ultimately means changing which thread is currently running on the processor, and thus
changing where processing time is being spent. Note that we focus our analysis on shared servers
where the server doesn't trust its clients, and the clients do not trust each other. Trusted
scenarios do not require this level of encapsulation: user-level threads packages can implement
locking protocols with libraries such as \texttt{pthreads}. Clients on the other hand must trust the
server if they share resources over synchronous \gls{IPC}, as the server can choose to never reply
to the client.

Access to shared servers encapsulating shared resources requires
cross-address-space IPC. For minimising invocation cost, it is
essential that scheduler invocations are avoided during IPC. This has
historically been done by L4
microkernels~\citep{Heiser_Elphinstone_16}. Not bypassing the
scheduler is the main reason why IPC on most other systems is
significantly slower than that of L4 kernel, e.g.\ at least a factor
of four in CertiKOS~\citep{Gu_SCWKSC_16}.

Past L4 kernels avoided the scheduler by time-slice donation, where a
server could execute on the client's time slice. While fast, this
model is unprincipled and hard, if not impossible, to
analyse. For example, the time slice may expire during the server's
execution, after which the server will run on its own time slice.
This results in no proper accounting of the server's execution, and no temporal isolation, and
relates to the precision problems discuss in \cref{s:tick-v-tickless}.

The proposed model supports shared servers, including scheduler
bypass, in a principled way through \emph{scheduling context
donation}: A client invoking a server can pass its SC along, so the
server executes on the client's SC, until it replies to the
request. This ensures that the time consumed by the server is billed
to the client on whose behalf the server's work is done. If the scheduling context 
expires, the enforcement mechanism of timeout exceptions can be used to recover the server according 
to server specific policy.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{legend-full}
    \caption{Legend for diagrams in this section, an expanded version of \cref{f:legend-1}}
    \label{f:legend-2}
\end{figure}

\subsection{Prioritisation}

We indicated in \Cref{s:scs} that scheduling contexts are separate from thread control blocks in
order to support resource sharing. Additionally, unlike previous microkernel designs for \glspl{SC}, priority
remains an attribute of the execution context. Decoupling priority and scheduling context avoids
prior patterns where \gls{PIP} is enforced by the kernel. 

\gls{IPCP} maps neatly to this model: if server threads are assigned the ceiling protocol of all of
their clients, then when the \gls{IPC} rendezvous occurs and we switch from client to server, the
servers priority is used.

The main drawback of \gls{IPCP}, namely the requirement that all
lockers' priorities are known \emph{a priori}, is easy to enforce in a
capability-based system: The server can only be accessed through an
appropriate invocation capability, and it is up to the system designer
to ensure that such a capability can only go to a thread whose
priority is known or appropriately controlled.

The choice of \gls{IPCP} is intentional: if the scheduling context holds the priority, then \gls{PIP}
is forced by the mechanism. Since \gls{PIP} has a known high preemption overhead, this would violate
our criteria of policy freedom and efficiency.  
Although \gls{OPCP} offers greater processor utilisation than \gls{IPCP}, we consider its use impractical as it requires too much system state to be drawn into the kernel: the kernel must track a system ceiling and implement priority inheritance.

However \gls{PIP} and \gls{OPCP} can used with this mechanism albeit in a more complex fashion, by
proxying requests to shared resources via a scheduling server which manipulates the priorities
appropriately. For \gls{PIP}, only one server per resource would be required and could exist in the
same address space as the resource. Since \gls{OPCP} requires global state, an implementation would
require a shared service for all threads sharing a set of resources.

\subsection{Charging}

\begin{figure}
    \centering
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{passive1}
        \caption{Phase 1}
        \label{f:passive1}
    \end{subfigure}%
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{passive2}
        \caption{Phase 2}
        \label{f:passive2}
    \end{subfigure}
    \label{f:passive}
    \caption{IPC phases between an active client and passive server: (a) shows the initial IPC rendezvous, (b) shows the
    reply phase. The client's $SC_{A}$ is donated between client and server. See \Cref{f:legend-2} for the legend.}
\end{figure}


Unlike prior implementations of scheduling contexts discussed in \cref{s:sc-intro},
donation is not compulsory, which provides more policy freedom than prior models.
However it is also not optional: instead we infer whether donation is required by testing 
if the server is passive or active. \emph{Passive} servers do not have a scheduling context 
and receive them over \gls{IPC} whereas \emph{active} servers have their own scheduling context. 
Passive servers, as illustrated in \cref{f:passive}, effectively provide a migrating-thread
model~\citep{Ford_Lepreau_94, Gabber_SBBS_99}, but without requiring
the kernel to manage stacks. \Cref{f:active} shows active servers, which allow system designers to
build systems without temporal isolation, as it is not suitable
for all systems.

Charging for time executed in a server becomes simple with this model: always charge the scheduling
context the server is executing on. The mechanism does not change regardless of the server being passive or
active. 

\begin{figure}
    \centering
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{active1}
        \caption{Phase 1}
        \label{f:active1}
    \end{subfigure}%
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{active2}
        \caption{Phase 2}
        \label{f:active2}
    \end{subfigure}
    \label{f:active}
    \caption{IPC phases between an active client and server: (a) shows the initial IPC rendezvous, (b) shows the
    reply phase. Both client and server have their own SC. See \Cref{f:legend-2} for the legend.}
\end{figure}

\subsection{Enforcement}

If a passive server exhausts its budget, it and any waiting clients
are blocked until the budget is replenished. On its own, this means that a client
not only has to trust its server, but all the server's other
clients. This would rule out sharing a server between clients of
different criticality.

In a \gls{HRT} system, we can assume that a client reservations are sufficient to complete
requests to servers.  However, in systems with best-effort and soft real-time tasks, no such
assumption can be made and client budgets may expire during a server request.  This leaves the
server in a state where it cannot take new requests as it is stuck without an active reservation to
complete the previous request.  Without a mechanism to handle this event the server, and any
potential clients, would be blocked until the client's budget is replenished.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{timeout}
    \caption{Example of a timeout exception handler. The passive server $S$ performs a request on
    client $A$'s scheduling context, $SC_{A}$. A timeout fault is generated when $SC_{A}$ is
exhausted and send to the servers timeout fault endpoint $E_{2}$, and the timeout handler receives it. See legend \cref{f:legend-1}}
    \label{f:timeout}
\end{figure}

Timeout exceptions can be used to remove this need for trust, and
allow a server to be shared across criticalities, as depicted in \cref{f:timeout}. Timeout fault endpoints are specific to the
execution context of a thread, not the scheduling context. Consequently, servers may have timeout
fault handlers while clients do not. The 
server's timeout handler can, for example, provide an emergency budget
to continue the operation (useful for \crit{HI} clients) or abort
the operation and reset or roll back the server. The latter option is
attractive for minimising the amount of budget that must be reserved
for such cases.

A server running out of budget constitutes a protocol violation
by the client, and it makes sense to penalise that
client by aborting. Helping schemes, such as PIP or bandwidth
inheritance,
make the waiting client pay for
another client's contract violation. This not only weakens temporal isolation,
it also implies that the size of the required reserve budget
must be a server's full WCET. This places a restriction on the server
that no client request exceed a blocking time that can be tolerated by
all clients, or that all clients must budget for the full server WCET in
addition to the time the server needs for serving their own request.
Our model provides more flexibility: a server can use a timeout
exception to finish a request early (e.g.\ by aborting), meaning that clients can only be
blocked for the largest budget of the other clients (plus the short
cleanup time).

\subsection{Cross-core IPC}

\TODO{diagram of cross core IPC}

Scheduling context donation extends naturally to IPC and allows users to specify active servers on
remote cores, or passive servers which migrate to the local core. The mechanism is simple: if a
server is passive, it migrates to the core that the donated scheduling context is on. 

\subsection{Resource sharing policies}

We now describe three policies for resource sharing over \gls{IPC} --- best-effort, helping and
isolation --- and describe how they can be implemented 
with our mechanisms of scheduling contexts, scheduling context donation and timeout exceptions.

\TODO{A diagram of each of these policies}
\TODO{Finish this section}
\subsubsection{Best effort}
\label{sec:best-effort}

Best effort systems have no timing requirements, so each thread in the system has a full \gls{SC}
with a budget and period the length of the timeslice. Threads are scheduled round robin. Server
threads are all active, and at the ceiling priority of the clients. 

This policy does not provide temporal isolation, as the server executes on its own budget.  If one
client launches a denial of service attack on the server, depleting the servers budget, then other
clients are starved. 

While our resource sharing mechanisms do not rule out this policy, it is only suitable for
systems where all clients are trusted and the amount of requests each client makes is known \emph{a
priori}, or systems that have lax temporal requirements.

\subsubsection{Migrating threads}

\composite~\citep{Parmer_10} solves the resource sharing model by using migrating threads (also termed stack-migrating IPC).
On every IPC, client execution contexts (and CPU reservation) transfer to a new stack running in the servers protection domain, resulting in multi-threaded servers.

To implement migrating threads, \composite requires that every server have a mechanism for allocating stacks.
If no memory is available to allocate stacks then the request is blocked.
This solution forces servers to be multi-threaded, and does not solve the problem of a clients budget expiring while the server is in a critical section, which is solved by providing atomic primitives that call the kernel.

While our model does not require this, it is possible to create a server which allocates execution
contexts should the existing server threads all be busy. Server threads would wait on an endpoint,
and a stack spawning thread would wait on the same endpoint with low-priority. The stack spawning
thread is only woken if all the server threads are busy, and otherwise does not run. An
alternative policy is to build systems with a fixed number of server threads. 

Like the best-effort policy, our model supports this approach but it is not required.

\subsubsection{Server isolation}
\TODO{Address peters comment: This is essentially an illustration for the preceding section, so
title it as such}

In a mixed criticality system, clients of different criticalities may share a server such as an
encryption service. Consider such a service which encrypts data in blocks. Clients can request
arbitrary length blocks to be encrypted. Using a passive server, the server can work in blocks, and
use a transaction based approach to alter between clean and dirty state. If a timeout exception
occurs, because a clients scheduling context has expired, the timeout exception handler can rollback
to the last clean state and reply to the client with the amount of data encrypted. The rollback has
a known complexity, so any pending requests need only to be able to cope with the cost of the
rollback, assuming the scheduling contexts are configured with a schedulable time allocation.


\section{Summary}

In this chapter we have outlined our model for providing temporal isolation via scheduling and
resource sharing mechanisms. We introduce support for user-level
admission tests and add processor reservations to the kernel in the form of scheduling contexts.  
Scheduling contexts differ from prior work in that they are decoupled from priority, thereby
avoiding the requirement of \gls{PIP}, and by differentiating between passive and active servers we
maintain policy freedom.

Finally, we outlined existing models for integrating real-time resource policies over \gls{IPC} and
show how scheduling context donation combined with passive servers can be used to create trusted
servers with untrusted clients. 
Our model supports best-effort, migrating threads, or temporal isolation policies for resource sharing.

Timeout exceptions are provided which allow for user-defined enforcement policies, while the default
mechanism maintains isolation by preventing threads from executing for than the share of processing
time represented by the scheduling contexts.

In the next section we will outline the implementation of our model in \selfour. 
