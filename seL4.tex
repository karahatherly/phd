

\chapter{\selfour Basics}
\label{chap:sel4}

So far we have provided a general background on real-time scheduling and resource sharing.
As the final piece of background we now present an overview of the concepts relevant to the temporal behaviour of our implementation platform, \selfour.

\selfour is a microkernel that is particularly suited to safety-critical, real-time systems with one
major caveat: time is not treated as a first class resource, leading to deficiency in real-time scheduling support. 
Three main features of \selfour support this claim: it has been formally verified for correctness~\citep{Klein_EHACDEEKNSTW_09} and other properties~\citep{Sewell_WGMAK_11}; All memory management, including kernel memory, is all at user-level~\citep{Elkaduwe_Derrin_06}; Finally it is the only \gls{OS} to date with full \gls{WCET} analysis~\citep{Blackham_SCRH_11}.
The scheduler in \selfour has been left intentionally underspecified~\citep{Petters_EH_12} for later work and as a result has a very informal notion of time.
The current implementation is a placeholder, and follows the traditional L4 scheduling model~\citep{Ruocco_06} --- a fixed-priority, round-robin scheduler with 256 priorities.

In this section we will present the current state of relevant \selfour features in order to highlight deficiencies and motivate our changes.
We will outline the existing scheduler, the API curiosity that is \yield, and how \gls{IPC} interacts with scheduling, followed by an analysis of how the current mechanisms can be used in real-time systems.


First we introduce the basics of \selfour: kernel memory management, capabilities, \gls{IPC} and
signalling. \Cref{f:legend-1} shows the legend for diagrams in this section. 

\section{Capabilities}
\label{s:capabilities}

As a capability-based \gls{OS}, access to any resource in \selfour is via capabilities (recall
\Cref{s:os-capabilities}). Capabilities to all system resources are passed to the initial task---the first
user-level thread started in the system---which can then allocate resources as appropriate.
Capabilities exist in a \emph{capability space} that can be configured per thread or shared between
threads. 

Capability spaces (\code{cspace}s) are analogous to address spaces for virtual memory: where address spaces map
virtual addresses to physical addresses, capability spaces map object identifiers to access rights.
Cspaces are formed of \emph{capability nodes} \code{cnode}s which contain capabilities, analogous to page tables
in virtual memory, and can contain capabilities to further \code{cnode}s, which allows for multi-level
cspace structure. A cspace address refers
to an individual entry in some CNode in the capability space, and may be empty or contain a
capability to a specific kernel resource. For brevity, a cspace address is referred to as
a \emph{slot}. 

% access rights
Each capability has three potential access rights: read, right and grant. How those rights affect
the resource the capability provides access to depends on the type of resource, and is explained in
the next section.

% badges and operations
Various operations can be done on capabilities, which are summarised in \cref{t:capability_ops}.
When a capability is copied or minted, it is said to be \emph{derived} from the original capability.
All derived capabilities can be deleted by using \code{revoke}.
There are restrictions on which capabilities can be derived and under what conditions, depending on
what the capability provides access to. 
\emph{Badging} is a special type of derivation which allows specific capability types to be copied
with an unforgeable identifier. We discuss derivation restrictions and the use of badges further
in this chapter.
Any individual capability can be deleted, or revoked. The former simply removes a specific
capability from a capability space, the latter removes all child capabilities.

\begin{table}
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabular}{l p{0.8\textwidth}}\toprule
    \emph{Operation}    & \emph{Description}\\\midrule
    \code{Copy}         & Create a new capability in a specified CNode slot, which is an exact copy
                         of the other capability and refers to the same resource. \\
    \code{Mint}         & Like copy, except the new capability may have diminished rights and/or be
                          badged. \\
    \code{Move}         & Move a capability from one slot to another slot, leaving the previous slot
                          empty. \\
    \code{Mutate}       & Like move, except the new capability may have diminished rights and/or be
                          badged. \\
    \code{Rotate}        & Atomically move two capabilities between three specified slots. \\
    \code{Delete}        & Remove a capability from a slot. \\
    \code{Revoke}        & Delete any capabilities derived from this capability. \\
    \code{SaveCaller}    & Saves the kernel generated resume capability into the designated slot. \\
    \bottomrule 
    \end{tabular}
    \caption{Summary of operations on capabilities provided by baseline \selfour~\citep{seL417}}.
     \label{t:capability_ops}
\end{table}

\section{System calls and invocations}

\selfour has a two-level system call structure, based on capabilities. The first level of system calls
are distinguishable by system call number, as listed in \cref{t:system-calls}. The majority of
system calls are for communication; \code{send}, \code{nbsend}, \code{call}, \code{reply} are for
sending messages; \code{recv}, \code{nbrecv} for receiving messages; and \code{yield} for
interacting with the scheduler. The \code{nb} prefix indicates that this system call will not block.

The second level of system calls are called \emph{invocations} and are modelled as sending a message
to the kernel. All invocations are conducted by a sending system call. The kernel is modelled as if
it is waiting for a message and receives one every time a system call is made, and sends a message
as a reply. 
To determine the operation, the rest of the arguments to an invocation are encoded as a message to
the kernel. Each capability type has a different set of invocations available, and on a send to the
kernel the capability is decoded to determine the action the kernel should take. 

\begin{table} 
    \centering
    \begin{tabular}{llll}\toprule
        \emph{System call}                         & Parameter     & Description                      & Return? \\\midrule
        \rowcolor{gray!25} \texttt{send, nbsend}   & \texttt{dest} & Capability to send a message to. & \no     \\
        \rowcolor{gray!25}                         & \texttt{info} & Information about the message
        sent. & \\     
        \texttt{call}              & \texttt{dest} & Capability to send a message to. & \\
                                   & \texttt{info} & Information about the message sent.   & \yes \\
        \rowcolor{gray!25}
        \texttt{recv, nbrecv}      & \texttt{src}  & Capability to wait for a message on. & \yes \\
        \rowcolor{gray!25}
                                   & \texttt{badge} & The kernel writes the badge of the sender
        here. & \\
        \texttt{reply}             & \texttt{info} & Information about the reply message sent. & \no\\
        \rowcolor{gray!25}
        \texttt{replyrecv}         & \texttt{info} & Information about the reply message sent. &\yes\\
        \rowcolor{gray!25}
                                   & \texttt{src} & Capability to wait for a message on. & \\
        \rowcolor{gray!25}
                                   & \texttt{info} & Information about the reply message sent. & \\
        \rowcolor{gray!25}
                                   & \texttt{badge} & The kernel writes the badge of the sender
        here. & \\
        \texttt{yield}     & \no & \no \\
        \bottomrule
    \end{tabular}
    \caption{\selfour system call summary. All system calls except \code{yield} are based on sending
    and/or receiving messages. The \emph{return} column indicates if a system call returns a message
or not.}
    \label{t:system-calls}
\end{table}

All of the operations on capabilities are that are listed in \cref{t:capability_ops} are invocations
on \code{cnode} capability addresses. For example, to copy a capability, one uses \code{call} on a
\code{cnode}, and provides the invocation code for copy, as well as the arguments. In the case of
copy, one provides the slot of the capability being copied, in addition to the destination
\code{cnode} and
slot. 

\section{Physical memory management}

All kernel memory in \selfour is managed at user-level and accessed via capabilities
which is key to \selfour's isolation and security, but also essential for
understanding the complexity of kernel design. Additionally, this allows for the ultimate in policy
freedom: all resource allocation is done from user-level by those holding the appropriate
capabilities. Capabilities to kernel memory contain a physical address and a type which indicates
what type of memory is at that physical address. Options for different types are shown in
\cref{t:kernel_objects}. 

% table of object types 
\begin{table}
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabular}{l p{0.6\textwidth}}\toprule
    \emph{Object}    & \emph{Description}\\\midrule
    Untyped    & Memory that can be retyped into other types of memory, including untyped.\\
    CNode            & A fixed size table of capabilities. \\
    \Gls{TCB}        & A thread of execution in \selfour.\\
    Endpoint  & Ports which facilitate \gls{IPC}. \\
    Notification object & Arrays of binary semaphores.\\
    Page Directories     & Top level paging structure. \\
    Page Tables  & Intermediate paging structures.\\
    Frames       & Mappable physical memory. \\
    \bottomrule
    \end{tabular}
    \caption{Major memory object types in \selfour, excluding platform specific objects. For further detail
    consult the \selfour manual~\citep{seL417}}.
     \label{t:kernel_objects}
\end{table}

In the initial system state, capabilities to all resources are given to the first task started by
the system, the \emph{root task}. Then according to system policy the root task can divide up and
delegate system resources.  This includes capabilities to all memory, apart from the small section
of static memory used by the kernel. The kernel itself has a large, static \emph{kernel window}
initialised at boot time, which
consists of memory mapped such that it is directly writeable by the kernel. The kernel window size
is platform specific, but is 500MiB on all 32-bit platforms.  

\subsection{Untyped}

All memory starts as \emph{untyped} memory, and capabilities to all available untyped memory are placed in the
cspace of the root task on boot. Each untyped consists of a start address, a size, and a flag
indicating whether the untyped is writeable by the kernel or not. Memory reserved for devices and
memory outside the kernel window is not readable or writeable by the kernel: the rest is untyped
memory, free for use by the system. 

Untyped objects have only one invocation: \emph{retype}, which allows for large untyped objects to
split into smaller objects of a different size and type, including frames, page tables, cnodes, etc. 
While the majority of objects in \selfour have a platform-dependent size fixed at compile time, some
are sized dynamically at runtime, \eg untyped and CNodes, which can be any power of two size.

Any capability to memory---untyped or not---is a capability to a specific object in memory,
containing a pointer to that object. When retype is used to create sub-objects in an untyped, those
subsets of memory will not become available for retyping again until every capability to that object has been deleted, somewhat like reference counting pointers.

\Cref{t:untyped} shows an example initial memory layout for the \textsc{Sabre} platform, which has a
500MiB kernel window. Physical memory on this platform starts at 0x10000000, which is mapped into
the kernel address space at 0xe0000000. Physical addresses outside of this range are devices and
are not writeable by the kernel.
Capabilities to all of the available memory and devices are set up as untyped in the initial root task's
\code{cnode}. 

\begin{table} 
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabular}{lll} \toprule
        \emph{Start physical address} & \emph{End physical address} & \emph{Kernel virtual address} \\\midrule
    0x100000   & 0x2c00000   & \no \\
    0x10000000 & 0x10010000 & 0xe0000000 \\
    0x105c3000 & 0x2f000000 & 0xe105c3000 \\
    0x2f106400 & 0x2fdfc200 & 0xff106400 \\
    \bottomrule
    \end{tabular}
    \caption{Initial memory ranges on at boot time on the \textsc{Sabre} platform.}
    \label{t:untyped}
\end{table}


\subsection{Virtual Memory}

Page tables, intermediate paging structures and physical frames are all created by retyping
untyped objects. Page tables and frames have a set of architectural invocations including mapping, 
unmapping, and cache flushing operations.

\subsection{Thread control blocks}

\Glspl{TCB} represent an execution context and manage processor time in \selfour, and consist of a
base TCB structure and a \code{cnode}. 
The base TCB structure contains accounting information for scheduling and IPC in addition to the
register set and floating point context. 
The \code{cnode} contains capabilities that should not be
deleted while a thread is running
leveraging the fact that an object cannot be truly deleted until all capabilities to it are removed.
These capabilities include the top-level \code{cnode}, top-level page directory, and three
capabilities for \gls{IPC} which we cover in the \cref{s:endpoint} section. 
\cref{t:tcb_ops} shows the main invocations possible on TCB capabilities.

% table of tcb invocations
\begin{table}
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabular}{l p{0.8\textwidth}}\toprule
    \emph{Operation}    & \emph{Description}\\\midrule
        \code{Resume}               & Place a thread in the scheduler.\\ 
        \code{Suspend}              & Remove a thread from the scheduler.\\
        \code{WriteRegisters}       & Configure a threads execution context.\\
        \code{ReadRegisters}        & Read a threads execution context.\\
        \code{SetAffinity}          & Set the CPU on which this thread should run.\\
        \code{SetIPCBuffer}         & Set the page to use for the IPC buffer.\\
        \code{SetPriority}          & Set the priority of this TCB.\\
        \code{BindNotification}     & Bind this TCB with a notification object (see
        \cref{s:notification-binding}. \\
    \bottomrule 
    \end{tabular}
    \caption{Summary of operations on TCBs. Further operations are available that batch several
    setters to reduce thread configuration overheads.}.
     \label{t:tcb_ops}
\end{table}

\subsection{Endpoints}
\label{s:endpoints}

Endpoints are the general communication port used by \selfour for \gls{IPC}.
Any thread with a
capability to an endpoint can send and receive messages on that endpoint, subject to the access
rights. Endpoints are small, and consist of the endpoint badge, some state information and a queue
of threads blocked on the endpoint. We cover how endpoints interact with IPC in the upcoming
\cref{p:sel4_ipc}. 

\subsection{Notifications}

Notification objects are an array of binary semaphores used to facilitate asynchronous communication in \selfour, either from other threads via \texttt{send()} or from
interrupts. Notification objects consist of a queue of blocked threads and a word of information,
which contains the semaphore state. Further information on notifications is presented in
\cref{p:sel4_notifications}. 

\subsection{Consequences}

While the user-level management of kernel memory provides true policy freedom---there is no
policy required by the kernel on memory layout---this design is not without trade-offs. Ultimately,
user-level management of kernel memory is key to \selfour's isolation
guarantees as system designers can fully partition systems by using specific memory layouts and
avoid shared structures determined by the kernel itself. However, there are two major impacts on kernel 
design: back-pointers, and dynamic data structures. 

The fact that any capability may be deleted at any time means that any pointer between two memory
objects must be doubled, with pointers from each object to each other, as any object must be able to be
traversed on order to reach any other linked object. This is analogous to a doubly-linked list,
where for $O(1)$ deletion from any node in the list, the list must have pointers in both directions.
This increases performance of deletion but also doubles the memory required for each list node. 

Secondly, because of the policy freedom, the kernel has no limits on resources, meaning no memory
resource can be statically sized. Consequently, simple, static data structures, like arrays, cannot
be used in the kernel as no assumptions on memory layout can be made, meaning dynamic data
structures are mandated. Because the kernel cannot make assumptions about the location of memory
objects, specific optimizations are also not possible: it is up to the system designer to decide a trade-off
between isolation and efficiency in the memory layout of the system.

Both of these consequences make for restrictions on kernel design, which can be demonstrated through
the list of TCBs maintained by the kernel scheduler. Each priority in the scheduler has a
doubly-linked list of TCBs: although the scheduler itself only ever removes the head of the list,
which is $O(1)$ on a singly-linked list, a doubly-linked list is required as TCB objects can be
deleted at any time. Memory placement of TCBs impacts scheduler performance, as depending on
allocation patterns, different list nodes may trigger cache misses or worse, cache conflicts. As a
result, the scheduler will perform far worse than a static, array-based scheduler using a fixed
maximum number of TCBs with known ids for indexing. However, such an approach provides no policy
freedom, and is more suitable at a middle-ware level in the \gls{OS} implemented on top of the
microkernel. 

\section{Control capabilities}
\label{s:control-capabilities}

Not all capabilities refer to memory-based resources, such as interrupts and \IO ports.
In order to obtain capabilities to specific interrupts or ranges of \IO ports, the root task is
provided with non-derivable control capabilities which can be invoked to place specific hardware
resource capabilities in empty slots.

\begin{table}
	\centering
    \rowcolors{2}{gray!25}{}
	\begin{tabular}{lll} 
        \toprule
        \emph{Start}  & \emph{End} & \emph{Contents} \\\midrule
        0                &               & empty \\
        1                &       & initial thread's TCB \\
        2                &       & this \code{cnode} \\
        3                &        & initial thread's page directory \\
        4                &        & \code{irq\_control} \\
        5                &        & \code{ASID\_control} \\
        6                &        & initial thread's ASID pool \\
        7                &        & \code{IO\_port\_control} \\
        8                &        & \code{IO\_space\_control} \\
        9                &        & initial thread start-up information frame \\
        10                &        & initial thread's IPC buffer frame \\
        11               &        & \code{domain\_control} \\
        12               & 18       & initial thread's paging structures \\
        19               & 1431     & initial thread's frames \\
        1431             & 1591     & untyped \\
        1592             & $2^{16}-1$ & empty \\
        \bottomrule
	\end{tabular}
    \caption{Slot layout in the initial \code{cnode} set up by the kernel for the root task on the
\textsc{Sabre}.}
	\label{t:initial_cnode}
\end{table}

\cref{t:initial_cnode} shows the root tasks initial cspace layout as set up by the kernel for the
\textsc{Sabre} platform. Apart from capabilities to memory objects for the root task and the
remaining untyped, the initial \code{cnode} contains five control capabilities for managing
interrupts, \glspl{ASID}, \IO Ports, \IO Spaces and domains.

We take interrupts as an example to explain how control capabilities function. The
\code{irq\_control} capability is the control capability for obtaining capabilities to specific
interrupt numbers, which is achieved by invoking the \code{irq\_control} capability.
Users provide an interrupt number and an empty slot and on success a IRQHandler capability to
that interrupt number is placed in that slot. Once this authority is granted, the IRQHandler can then be
invoked to manage that specific interrupt. On x86, \code{irq\_control} is also used to provide
capabilities to model specific registers and \IO interrupts.

\section{Communication}

\selfour provides communication through synchronous IPC via endpoints, or asynchronous notifications
via notification objects. All of the communication system calls introduced in \cref{t:system-calls},
when used on endpoints and notifications, are used to communicate between TCBs. There are no kernel
invocations that can take place via endpoints or notification capabilities. 

\subsection{IPC}
\label{p:sel4_ipc}

IPC in \selfour consists of threads sending and receiving messages in a blocking or non-blocking
fashion over endpoints, which act as
message ports. Each thread has a buffer (referred to as the \emph{IPC buffer}), which contains the payload of the message, consisting
of data and capabilities. Senders specify a message length and the kernel copies this (bounded)
amount between the sender and receiver IPC buffer. Small messages are sent in registers and do not
require a copy operation. 
Along with the message the kernel additionally delivers the badge of the endpoint capability that the sender 
invoked to send the message.

IPC can be one-way, where a single message is sent between a sender and receiver, or two-way in an
RPC fashion where the sender sends a message and expects a reply. \emph{IPC rendezvous} refers to
when the IPC takes place, specifically when the kernel transfers data and capabilities between two
threads. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{endpoint}
    \caption{State diagram of a single endpoint, where \emph{blocked} tracks the number of threads
    waiting to send or receive. Note that only one list of threads is maintained by the endpoint:
senders and receivers cannot be queued at the same time.}
    \label{f:endpoint}
\end{figure}

Table \cref{t:endpoint-system-calls} summarises \selfour system calls when used on endpoint
capabilities. Essentially, \code{send()} is used to send a message and \code{recv()} used to
receive one, both having blocking and non-blocking variants. \code{call()} is of particular interest as it represents the caller side of an RPC
operation critical to microkernel performance, distinguished from a basic pair of \code{send()} and
\code{recv()} by \emph{resume} capabilities. Pioneered in EROS~\citep{Shapiro_SF_99}, resume
capabilities are generated once the message sent by \code{call()} is consumed, and consist of the
thread waiting for a reply message. In \selfour, the resume capability is stored in the \gls{TCB}
\code{cnode}, and \code{reply()} is the operation used to invoke this capability and send the reply
message back.

\begin{table} 
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabular}{p{0.2\textwidth}p{0.8\textwidth}}\toprule
        \emph{System call} & Action \\\midrule
        \texttt{send}   & Send a message, blocking until it is consumed. \\ 
        \texttt{nbsend} & Send a message, but only if it is consumed immediately (\ie a thread is
        already waiting on this endpoint for a message).  \\
        \texttt{recv}   & Block until a message is available to be consumed from this endpoint.  \\
        \texttt{nbrecv} & Poll for a message---consume a message from this endpoint, but only if it is available
        immediately.   \\
        \texttt{call}   & Send a message, and block until a reply message is received. \\
        \texttt{reply}  & Send a reply message to a \code{call}.   \\
        \texttt{replyrecv} & Send a reply message to a \code{call} and then \code{recv}. \\
        \bottomrule
    \end{tabular}
    \caption{System calls and their effects when used on endpoints.}
    \label{t:endpoint-system-calls}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{legend-1}
    \caption{Legend for diagrams in this section}
    \label{f:legend-1}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ipc1}
        \caption{Initial IPC rendezvous}
        \label{f:ipc1}
    \end{subfigure}%
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{ipc2}
        \caption{Reply phase}
        \label{f:ipc2}
    \end{subfigure}
    \caption{IPC phases: a TCB, $A$ sends a message to endpoint $E$ using \code{call()}. Another
        TCB, $S$, blocks on $E$ using \code{recv()}. At this point the message is transferred from 
        $A$ to $S$ and $A$ is blocked the reply capability. (b) shows the
    reply phase, where $S$ uses \code{reply()} to send a reply message to $A$, waking $A$. See \Cref{f:legend-1} for the legend.}
    \label{f:ipc}
\end{figure}

\Cref{f:ipc1} demonstrates the rendezvous phase, where regardless of the order of operations, 
when one thread blocks (\code{recv()}) on the endpoint and another thread sends on that endpoint
then the message is consumed by the receiver. This occurs for both one-way and two-way \gls{IPC}.
Receivers can save
the \emph{resume} capability into their \code{cspace} to send a reply to later, but otherwise the resume capability is
installed in the \gls{TCB} CNode. The \emph{reply} system call directly invokes the resume
capability in this slot. 

Multiple senders and receivers can use the same endpoint, and which act as \gls{FIFO}
queues. In order to distinguish senders, receivers can use endpoint badges, which are unforgeable as
they are copied by the kernel directly. 

% fastpath
\subsection{Fastpath}

Recall from \cref{s:background-ipc} that \gls{IPC} performance is critical to a microkernel, which
is achieved through optimised fastpaths. A fastpath is an optimised code path for the most
performance critical operations. \selfour
contains two \gls{IPC} fastpaths which is used when the following, common-case conditions are satisfied:

\begin{enumerate}
    \item the sender is using \texttt{call()} or the receiver is using \texttt{replyrecv()},
    \item there are no threads of higher priority than the thread being woken in the scheduler,
    \item the thread to be switch to has a valid address space and has not faulted,
    \item and the message fits in registers.
\end{enumerate}

\subsection{Fault handling}

Fault handling in \selfour is modelled as IPC messages between the kernel and receivers. 
\glspl{TCB} can have a specific fault endpoint registered, on which the kernel can send simulated
\gls{IPC} messages containing information about the fault. Fault handling threads receive messages
on this endpoint as if the faulting thread had sent a message to that thread with \code{call()}. Of
course, this message is actually constructed by the kernel, and the message contains information
about the fault, generally the faulting \gls{TCB}'s registers. The faulting thread is blocked on the
resume capability, which is generated, just as if the faulting thread had conducted a \code{call()}.
The fault handling thread can subsequently reply to this message to
resume the thread, although the reply message is also special: it can be used to reply with a new
set of registers for the faulting thread to be resumed with, and tell the kernel if it should
restart the thread or leave it suspended after the reply message is processed.
If no fault endpoint is present, the
thread is rendered inactive and no fault message is sent. 

\subsection{Notifications}
\label{p:sel4_notifications}

Notification objects provide the mechanism for semaphores in \selfour, and consist of a word of
data. Sending on a notification either sends the badge of the invoked notification capability to a 
thread waiting on that notification object, or if no threads are waiting stores the badge in the
data word of the notifications. Unlike IPC, where messages being send are queued, notifications
accumulate messages in the data word.

The data word is set when the first notification arrives, and further invocations continue to bitwise OR the badge and data word
until a thread receives the signal and clears the word. This is illustrated in the notification
state diagram depicted in \cref{f:notification}, and in \cref{f:signal1} which shows the
notification object and TCB interaction.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{notification}
    \caption{State diagram Notification object state transitions based on invocations. \code{send} and
    \code{nbsend} correspond to notify in the diagram, want wait corresponds to a \code{recv}.}
    \label{f:notification}
\end{figure}

\Cref{t:notification-system-calls} shows the operations that occur when notification objects are
invoked with relevant system calls. \cref{f:notification} depicts changes in the notification object
state that occur when threads notify and block on notifications. 

\begin{table} 
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabular}{p{0.2\textwidth}p{0.8\textwidth}}\toprule
        \emph{System call} & Action \\\midrule
        \texttt{send}   & Send a notification, transmitting the badge, do not block. \\ 
        \texttt{nbsend} & As above. \\
        \texttt{recv}   & Wait until a notification is available then receive the data word.  \\
        \texttt{nbrecv} & Poll for a notification, do not block, receive the data word if available. \\
        \bottomrule
    \end{tabular}
    \caption{System calls and their effects when used on endpoints.}
    \label{t:notification-system-calls}
\end{table}


\subsubsection{Interrupts}

In addition to providing a mechanism for threads to notify each other, notification objects 
also allow threads to synchronise with devices via polling and/or blocking for interrupts. 
IRQHandler capabilities can be associated with a single notification object, via the invocation
\code{set\_notification}, which results in the kernel issuing a notification when an interrupt
occurs. The badge of the notification capability provided to the invocation is bitwise ORed with the
data word when an interrupt is triggered. Further notifications are not issued by the kernel
until the interrupt is acknowledged, using the \code{acknowledge} invocation on IRQHandler
capabilities.

\subsubsection{Notification binding}
\label{s:notification-binding}

Some systems require threads that can receive both notifications and IPC while blocked, in order to
prevent the requirement that services which receive both IPC message and notifications be multi-threaded.
The
mechanism for this is \emph{notification binding} where threads can register a specific notification
specific notification capability to receive notifications from while blocked on
an endpoint waiting for IPC. This is done by invoking the TCB
with the \code{bind\_notification} invocation, which establishes a link between a TCB and
notification object. Subsequently, if a notification is sent on that notification object and the TCB receives
on any endpoint, that TCB will receive the notification. 
Without notification binding, services require a thread for blocking on a notification and another
thread for blocking on an endpoint, both threads must then synchronise carefully on any shared data.
Notification binding is illustrated in
\cref{f:signal2}.


% notifications, interrupts, aep-binding
\begin{figure}
    \centering
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{signal1}
        \caption{Notifications.} 
        \label{f:signal1}
    \end{subfigure}%
    \begin{subfigure}[h]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{signal2}
        \caption{Notification binding.}
        \label{f:signal2}
    \end{subfigure}
    \caption{Example of a thread, TCB $A$, receiving notifications, by blocking on the notification and by
        notification binding. In \cref{f:signal1} $A$ blocks waiting on notification
        object $N$, and wakes when any notifications or interrupts are sent to $N$. In
        \cref{f:signal2}, $A$ blocks on endpoint $E$, however since
        $N$ is bound to the $A$, if $N$ receives an interrupt or notification,
        $A$ is woken and the data word delivered to $A$.
        See \Cref{f:legend-1} for the legend.}
    \label{f:signal}
\end{figure}

\section{Scheduling}

The scheduler in \selfour is used to pick which runnable thread should run next on a specific
processing core, and is a priority-based round-robin scheduler with 256 priorities (0---255). 
At a scheduling decision, the kernel chooses the head of the highest-priority, non-empty list.

Implementation wise, the scheduler consists of an array of lists: one list of ready threads for each
priority level. A two-level bit-field is used to track which priority lists contain threads, 
in order to achieve a scheduling decision complexity of $O(1)$. 




field is used to 
Scheduling decision complexity is $O(1)$ as a two-level bit field tracks the highest priority with a runnable thread.

h
and
removes it from the relevant scheduler queue, which is referred to as \emph{Benno Scheduling}, named
after its original author.
T





\begin{figure}[h!tb]
    \centering
    \includegraphics[width=0.7\textwidth]{thread_state}
    \caption{Thread state transitions in \selfour}
    \label{f:thread_state}
\end{figure}



% scheduler
The scheduler in \selfour is priority-based round-robin with 256 priorities (0 --- 255), implemented ase kernel previously used \emph{lazy scheduling}, leaving the current thread in its run queue, however this was replaced in favour of Benno scheduling to reduce the WCET of the kernel. 


% domains

% thread states


\subsection{Thread states}

\begin{description}
    \item \code{Running} This thread is eligible to be picked by the scheduler, and should be in the
        scheduling queues or be the currently running thread. 
    \item \code{Inactive} This thread is not runnable, and is not in the scheduler. It has been
        suspended, or possibly never resumed. 
    \item \code{BlockedOnRecv} This thread is waiting to receive an IPC (or bound notification). 
    \item \code{BlockedOnSend} This thread is waiting to send an IPC. 
    \item \code{BlockedOnReply} This thread is blocked on a resume capability, waiting for a reply
        from a \code{call} or fault.
    \item \code{BlockedOnNotification} This thread is waiting to receive a notification.
\end{description}

\Cref{f:thread_state} depicts a simplified state diagram of the thread states that
threads transition through, although in the diagram we map multiple blocked states to
\code{BlockedOnObject} for simplicity.  




As depicted in \Cref{f:thread_state}, threads in \selfour can be inactive, running or blocked on a
specific object. The thread state encodes the object which the thread is blocked on. 



Kernel time is accounted for in ticks, with a static tick length defined at compile time (CONFIG\_TIMER\_TICK\_MS).
Threads have a timeslice field which represents the number of ticks they are eligible to run. 
This value is decremented every time a timer tick is handled, and when the timeslice is exhausted the thread is appended to the relevant scheduler queue, with a replenished timeslice.
The reload value of the timeslice is also defined at compile time (CONFIG\_TIME\_SLICE).

In an unrealistically simple system, where threads run at the same priority and do not communicate, it is possible to analyse temporal behavior on \selfour: threads will run for the timeslice value in round-robin order.
However, the allocation of ticks to threads is not actually that simple due to yield behaviour, preemption and \gls{IPC}. 

\subsection{Priorities}

Like any priority-based kernel without temporal isolation mechanisms, time is only guaranteed to the highest priority threads.
Priorities in \selfour act as informal capabilities: threads cannot create threads at priorities higher than their current priority, but can create threads at the same or lower priorities.
If threads at higher priority levels never block, lower priority threads in the system will not run.
As a result, a single thread running at the highest priority has access to 100\% of processing time.
However, even this becomes unclear once there is more than one thread at a priority: if two threads are running, they can both access 50\% of processing time.
If one of two threads blocks, the other gets 100\% of processing time and vice versa.
There is no way to enforce a certain processor allocation and how CPU time a thread receives is up
to priorities and the behaviour of other threads in the system, which is impossible to guarantee.

\subsection{Accounting}
% I think this should move to the design section?
\selfour has a tick-based scheduler, where ticks are accounted to the currently running thread,
meaning that temporal isolation in servers is not possible and accuracy is traded for precision, as
discussed in \Cref{s:tick-v-tickless}.

Similarly, the \texttt{yield} system call will not alter the timeslice of the current thread, and
only donates a portion of the current tick to the next thread in the round-robin scheduler. 

\subsection{Domain scheduler}

A recent addition to the \selfour kernel adds the ability to guarantee complete temporal isolation and deterministic scheduling between sets of threads, using the concept of scheduling \emph{domains}.
Threads are assigned to domains, each of which has a separate array of lists of threads over the priority range.
Each domain runs for a fixed amount of ticks, and domains are scheduled sequentially and deterministically.
Cross-domain \gls{IPC} is delayed until a domain switch, and \texttt{yield} between domains is not
possible. When there there are no threads to run while a domain is scheduled, a domain-specific idle thread will run until a switch occurs.

The domain scheduler can be leveraged to achieve temporal isolation however since domains cannot be
preempted, it is only useful for cyclic, non-preemptive scheduling with scheduling order and
parameters computed-offline.
In such a scenario each real-time task could be mapped to its own domain, and each task would run for its specified time before the domain scheduler switched to the next thread.
Any unused time in a domain would be wasted, and spent in the idle thread.

Such a scheduler is only suitable for closed systems and results in low system utilisation.
Dynamic real-time applications with shared resources and high system utilisation are not compatible
with the domain scheduler, as they require preemption.

\section{RT support}

We introduced basic \selfour concepts and terminology, and investigated mechanisms that effect
timing behaviour in the kernel: the scheduler, priorities, yield and IPC. 
In this section we will look at how real-time scheduling could be implemented with those mechanisms.

There are several options for implementing a real-time scheduler in the current version of \selfour: leveraging the domain scheduler, using the priority scheduler or implementing a scheduling component to run at user-level. 
The domain scheduler offers low utilisation for strictly closed
systems with strict temporal partitioning and no preemption, so is clearly insufficient, as
discussed in \Cref{sec:rt-scheduling}.

The priority scheduler could be leveraged to implement a rate-monotonic scheduler.
However, this requires complete trust in every thread in the system, as there is no mechanism for temporal isolation: if one thread executes for too long, other threads will miss their deadlines.
Essentially the only thread with a guaranteed CPU allocation is the highest priority thread, which under rate-monotonic priority assignment is not the most critical thread in the system, but the thread with the highest rate.
Additionally, periodic threads driven by timer interrupts rather than events would need to share a user-level timer.

\begin{table}
	\centering
	\begin{tabular}{lp{2cm}p{2cm}p{2cm}p{2cm}} \toprule
        & \emph{Temporal isolation} & \emph{Utilisation} & \emph{Low kernel overheads} &
        \emph{Dynamic}\\
        \midrule
Domain scheduler          & \yes               & \no         & \yes        & \no    \\
Priority scheduler        & \no                & \yes        & \yes        & \yes   \\
Trusted timer component   & \yes               & \yes        & \no         & \yes   \\
        \bottomrule
	\end{tabular}
	 \caption{Comparison of existing \selfour scheduling options -- nothing ticks all of the boxes.}
	 \label{tab:nothing-ticks-all-boxes}
\end{table}


To build a dynamic system with temporal isolation and high CPU utilisation, one could implement a trusted timer component at user-level.
This component would be the highest priority thread in the system, and could pause threads to prevent them from overrunning their assigned rate.
However, since the timer component would need to maintain accounting information and track the currently running thread, it would need to be invoked for every single scheduling operation.
This is prohibitively expensive, as it results in doubled context switching time and increased number of system calls for thread management.

\Cref{tab:nothing-ticks-all-boxes} shows a comparison of all of the available scheduling options in the current version of \selfour -- no option provides all of the qualities we require.
Clearly, the kernel needs something more. 
In the next section we will our model for a more principled approach to time by extending the
baseline \selfour model presented in this chapter. We incorporate using the principles of resource kernels and with the aim of support diverse task sets, including those for mixed-criticality systems.

