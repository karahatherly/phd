

\chapter{Operating Systems}
\label{chap:operating-systems}

In this chapter we provide a survey of existing operating systems and mechanisms for building
mixed-criticality systems. Although not all operating systems support mixed-criticality, we evaluate
the scheduling and resource sharing policies and mechanisms available, with a focus on temporal isolation,
asymmetric protection, and resource sharing. 

First, we look at the \gls{POSIX} standard which impacts many commercial and open source operating
systems. Then we examine what is available in Linux, before presenting common microkernel
mechanisms and \glspl{OS}.

\section{POSIX}

The \gls{POSIX} standard specifies \gls{RTOS} interfaces~\citep{Harbour_93} which
influence a large amount of \gls{OS} design.  Scheduling policies specified by \gls{POSIX} are shown in
\Cref{tab:posix-sched}. 

\begin{table}
\centering
\rowcolors{2}{gray!25}{white}
\begin{tabular}{lp{.7\textwidth}}\toprule
    \emph{Policy}  & \emph{Description} \\\midrule
    \schedfifo     & Real-time tasks can run at a minimum of 32 fixed-priorities until they are preempted or yield. \\
    \schedrr       & As per \schedfifo but with an added timeslice. If the timeslice for a thread expires, it is added to the tail of the scheduling queue for its priority.\\
    \schedsporadic & Specifies sporadic servers as described in \Cref{p:sporadic} and can be used
    for temporal isolation. For practical requirements, the POSIX specification of \schedsporadic
    specifies a maximum number of replenishments which is implementation defined. \\\bottomrule
\end{tabular}
\caption{\gls{POSIX} real-time scheduling policies}
\label{tab:posix-sched}
\end{table}

\citet{Faggioli_08} provides an implementation of \schedsporadic, which \citet{Stanovic_BWH_10}
use to show that the POSIX definition of the sporadic server is incorrect and can allow tasks to
exceed their utilisation bound.  The authors provide a modified algorithm for merging and abandoning
replenishments which fixes these problems, of which corrections to the pseudo code were published in
\citet{Danish_LW_11}.  In further work \citet{Stanovic_BW_11} show that while sporadic servers
provide better response times than polling servers under average load, under high load the overhead
of preemptions due to fine-grained replenishments causes worse response times when compared to
polling servers.  Consequently they evaluate an approach where servers alternate between sporadic
and polling servers depending on load, where the transition involves reducing the maximum number of
replenishments to one and merging available refills.

Resource sharing in the \gls{POSIX} \gls{OS} interface is permitted through mutexes, which can be
used to build higher synchronisation protocols.  \Cref{tab:posix-mutex} shows the specified
protocols. 

\begin{table}
\centering
\rowcolors{2}{gray!25}{white}
\begin{tabular}{lp{.7\textwidth}}\toprule
\emph{Policy} & \emph{Description} \\\midrule
\noprioinherit & Standard mutexes that do not protect against priority inversion. \\
\prioinherit  & Mutexes with \gls{PIP} to prevent priority inversion, recall \Cref{sec:pip}. \\
\prioprotect & Mutexes with \gls{HLP} to prevent priority inversion, recall \Cref{sec:hlp}. \\
\bottomrule
\end{tabular}
\caption{\gls{POSIX} real-time mutex policies for resource sharing.}
\label{tab:posix-mutex}
\end{table}

Although \gls{POSIX} provides \schedsporadic which can be used for temporal isolation (however
flawed), the intention of the policy is to contain aperiodic tasks.  Since \schedsporadic allows
tasks to run at a lower priority when they have exceeded their allowed budget in any given period,
it follows that the locking protocols \prioinherit and \prioprotect would continue to operate --
although the excess time is not billed to the task.  If a task never releases a locked resource,
temporal isolation is completed violated.  As a result, \gls{POSIX} is insufficient for
mixed-criticality systems where tasks of different criticalities share resources.  Few \glspl{OS}
implement the full \gls{POSIX} standard, however many incorporate features of it, including Linux.

\section{Linux}

While Linux cannot be considered a real-time operating system for high criticality applications, it
is frequently used for lowr criticality applications with \gls{SRT} demands.  Additionally, Linux
is often used as a platform for conducting real-time systems research. 
% TODO cite completely fair scheduler
Linux has fixed-priority preemptive scheduler which is split into scheduling classes.  Real-time
threads can be scheduled with \gls{POSIX} \schedfifo and \schedsporadic. Best effort threads are
scheduled with \gls{CFS}, and real-time threads are scheduled either \gls{FIFO} or round-robin, and
are prioritised over the best effort tasks.  Fixed priority threads in Linux are completely trusted:
apart from a bound on total execution time for real-time threads which guarantees that best effort
threads are scheduled (referred to as real-time throttling~\citep{Corbet_08}), individual temporal
isolation is not possible.

Linux version 3.14 saw the introduction of an \gls{EDF} scheduling class to Linux~\citep{Corbet_09},
which is between the fair and the fixed priority scheduling classes.  The \gls{EDF} implementation
allows threads to be temporally isolated using \gls{CBS}.

Scheduling in Linux promotes the false correlation we see in many systems: real-time tasks are
automatically trusted (unless scheduled with \gls{EDF}) and assumed to be more important, or more
critical, than best effort tasks.  In reality, criticality and real-time strictness are orthogonal.
Linux does not provide any mechanisms for asymmetric protection beyond priority.

On the resource sharing side Linux provides real-time locking via the POSIX  as per
\Cref{tab:posix-mutex}.
 
\subsection{Real-time Linux Extensions}

A large amount of projects exist that attempt to retrofit more extensive real-time features onto
Linux.  We briefly summarise major and relevant works here.  One of the original
works~\citep{Yodaiken_Barabanov_97} runs Linux as a fully preemptable task via virtualisation and
kernel modifications.  Interrupts are handled by the virtualisation layer, and only directed to
Linux if required.  This means that real-time tasks do not have to suffer from long interrupt
latencies, however it also means that devices drivers need to be rewritten from scratch for
real-time.

\litmus~\citep{Calandrino_LBDA_07} is an extension of Linux that allows for pluggable real-time
schedulers to be easily developed for testing multiprocessor schedulers.

Linux/RK~\citep{Oikawa_Rajkumar_98} is a resource kernel implementation Linux that is often used to
implement new scheduling algorithms.  \gls{ZS} scheduling~\citep{deNiz_LR_09} was implemented and
tested using Linux/RK.

Whilst Linux implementations are suitable for implementing algorithms, being used as test-beds and
even being deployed for non-critical \gls{SRT} applications, ultimately Linux is not a suitable
\gls{RTOS} for running safety-critical \gls{HRT} applications. The large amount of source code
results in a colossal trusted computing base,where it is impossible to guarantee correctness through
formal verification or timeliness through {\gls{WCET}} analysis.  Major reasons for adapting Linux
to real-time are the existing applications and wide array of device and platform support. For
mixed-criticality systems these advantages can be leveraged by virtualising Linux to run \gls{SRT}
and best effort applications.

\subsection{Other open-source RTOSes}

\citet{RTEMS:URL} is an open-source \gls{RTOS} that operates with or without
memory protection, although in either case it is statically configured.  Although it is an open
source project, RTEMS is used widely in industry and research.  The main scheduling policy is
\gls{FPRM}, however \gls{EDF} is also available with temporal isolation an option using \gls{CBS}.
No temporal isolation mechanisms are present for fixed-priority scheduling.  RTEMS provides
semaphores with \gls{PIP} or \gls{HLP} for resource sharing, as well as
higher level primitives for these. 

\citet{FreeRTOS:URL} is another open-source \gls{RTOS}, however it only supports systems with \glspl{MPU}, not
\glspl{MMU}. The scheduler is preemptive \gls{FP} and \gls{PIP} is provided to avoid priority inversion.
FreeRTOS can be configured to be tickless or not.
%TODO{Where does Free RTOS go??}

\section{Microkernels}

Microkernels, as introduced in \cref{sec:background-operating-systems} are small operating systems
kernels which contain the minimum amount of software to implement an OS. Real-time and resource
kernel concepts are not
new microkernels. In this section we present several existing mechanisms that exist in microkernels
that can be used to support mixed-criticality systems and survey existing microkernels. We look at
how different kernels address resource kernel concepts required to treat time as a first class
resource; scheduling, accounting, enforcement, admission. Additionally we look at how
prioritisation, charging and enforcement are achieved, if at all, to achieve temporal isolation
across shared resources.

%Separation kernels ??
\subsection{Scheduling}

The majority of research microkernels use fixed priority scheduling, including:

\begin{itemize}
    \item KeyKOS~\citep{Bomberger_FFHLS_92},
    \item real-time Mach~\citep{Mercer_RZ_94, Mercer_ST_93},
    \item DROPS~\citep{Haertig_BBHHMRSW_98}, 
    \item EROS~\citep{Shapiro_SF_99},
    \item Minix 3~\citep{Herder_BGHT_06}.
    \item L4 family kernels~\citep{Elphinstone_Heiser_13}, including \selfour~\citep{Klein_EHACDEEKNSTW_09},
    \item NOVA~\citep{Steinberg_Kauer_10},
    \item Fiasco O.C~\citet{Lackorzynski_WVH_12},
    \item Quest-V~\citep{Li_WCM_14}.
\end{itemize}

Fixed priority is also prevalent in industry, with microkernels including QNX
Neutrino~\citep{QNX_10}, VxWorks~\citep{VxWorks_08} and PikeOS~\citep{PikeOS:URL} all providing
\gls{POSIX} compliant schedulers that support the ARINC 653~\citep{ARINC653} standard.  

\gls{EDF} is used in Barrelfish~\citep{Peter_SBBIHR_10} and Tiptoe~\citep{Craciunas_KPRS_09}, while 
\composite~citep{Parmer:phd} does not provide a scheduler, merely a system call to switch threads,
such that schedulers can be implemented at user-level~\citep{Parmer_West_11}.

\subsection{Accounting \& Enforcement}

Simple \gls{FP} and \gls{EDF} do not provide the accounting and enforcement required to provide 
temporal isolation, and only guarantee deadlines are met if tasks declare their timing behaviour
and abide by that declaration. 

Tasks can be untrusted for many reasons including:
\begin{itemize}
    \item sporadic tasks with inter-arrival times that are event driven will not necessarily have
        device drivers which guarantee the inter-arrival time;
    \item the task may have an unknown or unreliable \gls{WCET};
    \item the system may be open and the task from an untrusted source;
    \item the task may be low criticality and therefore not certified to a high level.
\end{itemize}

Many of the microkernels presented above with fixed-priority schedulers
do not have temporal isolation mechanisms and rely on 
assigning high priorities to threads that are trusted. However this does not result in high 
processor utilisation, as trust does not correlate with the rate of a task, resulting in a
scheduling order that is not optimal. 

Therefore, accounting and enforcement techniques are required to ensure tasks 
assume it will surrender access to the processor voluntarily.

% processor capacity reserves
Real-time Mach~\citep{Mercer_RZ_94, Mercer_ST_93} first introduced the concept of
\emph{processor capacity reserves} which were later used in EROS~\citep{Shapiro_SF_99}. 
Reserves were implemented as deferrable servers (recall \cref{p:ds}) for temporal isolation, 
although the back-to-back problem presented in \cref{p:ds} prevents this approach from providing
full isolation.

Additionally, processor capacity reserves were optional: threads could be reserved or unreserved. 
This formed a two level scheduler, where reserved threads with available capacity were scheduled
first. Threads which had exhausted their reservation were able to be scheduled by a second level
time-sharing scheduler that scheduled all of the unreserved threads, if no depleted reservations
were present in the scheduler.  

L4 family microkernels have avoided temporal isolation and provided a simple timeslice, which is
when depleted causes a round-robin reschedule.

Sporadic servers are provided by
Quest-V~\citep{Li_WCM_14}, and QNX, while Barrelfish contains an \gls{RBED} implementation and
Tiptoe used a modified version of \gls{CBS}. 

\subsection{Resource sharing}

\subsubsection{\Glsentrylong{IPC}}

Before examining temporal isolation further, we first examine \gls{IPC}, which is integral to 
microkernel resource sharing. 

In a microkernel, \gls{OS} services can be implemented as user-level servers which are communicated
with by clients. These servers can be viewed as resources from the previous, and clients as the
tasks accessing those resources. \gls{IPC} is a critical function in microkernels, which in L4
family microkernels has traditionally avoided the scheduler for
performance~\citep{Heiser_Elphinstone_16}. Past L4 kernels avoided the scheduler by time-slice
donation, where a server could execute on the client's time slice. While fast, this model is
unprincipled and hard, if not impossible, to analyse. For example, the time slice may expire during
the server's execution, after which the server will run on its own time slice.  This results in no
proper accounting of the server's execution, and no temporal isolation.

\subsubsection{Scheduling contexts}

Scheduling contexts are a microkernel abstraction which can be mapped to reservations, but also used
to implement resource sharing. We describe them in detail here, including their history and
variations.  

Scheduling contexts are separate to an \emph{execution
context} (registers, address space \etc) and fields related to scheduling into two separate objects.
The scheduling context can then travel across \gls{IPC} from client to server, allowing proper
accounting of the clients use of the resource. The contents of the scheduling context, and how 
scheduling contexts and \gls{IPC} interact, vary depending on the implementation.  

Real-time Mach's processor capacity reserves were the first form of scheduling context, and along
with \gls{DS} parameters contained a priority, which would transfer over \gls{IPC} from
client to server. This maps directly on to bandwidth inheritance with \gls{PIP} with all the
downsides presented in \cref{p:pip}. 






% scheduling contexts
% how many, when to transfer, what prio, whats in an sc


% scheduling contexts

In Real-time Mach, scheduling context donation was compulsory where processor capacity reserves were
used to implement temporal isolation, however threads could be reserved or non-reserved threads.
Threads which had exhausted their reservation were able to be scheduled by a second level
time-sharing scheduler that scheduled all of the unreserved threads, if no depleted reservations
were present in the scheduler.  Real-time Mach used fixed-priority scheduling and rate-monotonic
analysis to conduct an admission test for reservations in the kernel.  If a server exhausted the
donated reservation, it would finish the operation using the time-sharing scheduler.  Finally,
Real-time Mach provided \gls{PIP} over \gls{IPC} to avoid priority inversion~\citep{Tokuda_NR_90}.


\subsection{Admission}

Kernels with a concept of more than a basic timeslice all conduct admissino tests in the kernel
itself, including: 

\subsection{Capabilities to time}
\label{s:capabilities}

Capabilities~\citep{Dennis_VanHorn_66} are an established mechanism for fine-grained access control
to spatial resources which allow for spatial isolation. A capability is a unique, unforgeable token
that gives the possessor permission to access an entity or object in system. Capabilities can have
different levels of access rights, e.g. read, write, execute etc.

% meters  
Capability systems have been applied to the control of processing time, first in 
KeyKOS~\citep{Bomberger_FFHLS_92} which had capabilities to \emph{meters}, which granted the holder
the right to execute for the unit of time held by the meter.
However, the KeyKOS model treats time as fungible, with no guarantee
of \emph{when} the time will be provided, which makes this approach
unsuitable for real-time use.

EROS~\citep{Shapiro_SF_99} applied capabilities to the processor capacity reserves of real-time
Mach. 



However, capabilities can also be applied to time. Although not a capability based \gls{OS}, 
Real-time Mach~\citep{Mercer_RZ_94, Mercer_ST_93} first introduced the concept of
processor capacity reserves. Threads could use a reserve or not, but


Real-time Mach~\citep{Mercer_ST_93} introduced
\emph{processor capacity reserves}, which were also implemented in EROS and combined with
capabilities~\citep{Shapiro_SF_99}.

However these reserves were optional: a two level scheduler first
scheduled the reserves with available capacity, then threads with no
or exhausted reserves were scheduled.
Like any hierarchical scheduling model, this enforces a policy that
reduces flexibility.


Many existing and research \glspl{OS} use capabilities for a variety of things. \composite provides
a capability system outside of the kernel as a user-level policy.

% TODO capabilities to time
% overview of time caps
\subsection{Timeslice Donation}

%TODO{diagrams}
%TODO{describe timeslice donation}
%TODO{and migrating threads}



\subsection{Scheduling contexts}

Many kernels distinguish between a threads execution context (the registers, stack \etc) and
scheduling context (access to processing time) in order to allow the scheduling context to transfer
between threads for accounting purposes.  This is a more explicit form of timeslice donation: when
the scheduling context is passed between client and server, the scheduling context of the client is
billed for activity on the servers behalf.  Designs differ as to whether scheduling context donation
is required or optional.  Some designs allow multiple execution contexts to be attached to one
scheduling context, such that the scheduling context forms a secondary run queue.  In others,
multiple scheduling contexts can be bound to one execution context, allowing the execution context
to execute on any scheduling context with available execution time. 

\citet{Steinberg_WH_05} scheduling context consisted of a time quanta and a priority and were
donated across \gls{IPC}.

\subsection{IPC}
%TODO{describe IPC, how it can be used to make resource servers}
% TODO picture of ipc, to build on and refer to later
% TODO performance of ipc, why its important
% TODO define fastpath here. 



\subsection{Examples}


\paragraph{EROS}~\citep{Shapiro_SF_99} is a capability based operating system designed to
demonstrate the viability of capability-based systems in terms of performance.  EROS introduced the
idea of a single-use reply capability, termed a \emph{resume} capability, which when invoked would
be consumed and allows the receiver to reply to the sender.  EROS also used what is known as an
\emph{entry} capability, which allowed the holder to invoke the services provided by a program
within a particular process.  EROS uses the same scheduling and resource sharing policies as
Real-time Mach, using capacity reserves, however these are not accessed via the capability system.

\paragraph{DROPS} (Dresden Real-time OPerating System)~\citep{Haertig_BBHHMRSW_98} is an L4 based
real-time microkernel that also takes the resource reservation route to temporal isolation.  The
scheduling scheme in DROPS allows a process to reserve a priority for an amount of cycles during a
time period.  Page colouring is used to reserve parts of the caches for real-time tasks,
significantly decreasing upper bounds on \gls{WCET}.

\paragraph{NOVA}~\citep{Steinberg_Kauer_10} is a capability-based microkernel aimed at virtualisation. 
NOVA provides fixed-priority round-robin scheduling, with priority inheritance across IPC~\citep{Steinberg_BK_10}.
Priorities in NOVA reflect importance: it is not a real-time kernel, and periodic scheduling is not available.  
NOVA provides \gls{BWI} through the microkernel mechanism of donation is used, so NOVA does not
provide concrete reservations, but when a timeslice expires other tasks waiting for the resource
\emph{help} the blocked task, where the blocked task executes on the pending tasks timeslice.
If the task holding the resource does not release it, all pending tasks will block forever.


\paragraph{Fiasco.O.C} is an L4 microkernel where scheduling, accounting, enforcement and admission
are all implemented in the kernel, with the motivation that these functions would have high
overheads if implemented at user-level.  Resource reservations are realised as \emph{scheduling
contexts} which act as timeslices: a budget paired with a priority, and a replenishment rule.  Some
scheduling mechanisms are exposed to the user;~\citet{Lackorzynski_WVH_12} alter the system call
interface to support multiple mixed-criticality guests.  They find that the only way to avoid
scheduling integrity violations is to export scheduling information to the host \gls{VM}.  Guests
can change scheduling contexts on priority switches such that the host schedules guests with enough
time to schedule all tasks.  Additionally, guests associate scheduling contexts to \glspl{ISR}.

An \gls{RBED} implementation has also been completed on OKL4~\citep{Petters_LHE_09}.
%TODO{more on OKL4}

\paragraph{seL4} is a microkernel that is particularly suited to safety-critical, real-time systems
with one major caveat: the lack of real-time scheduling support.  Three main features of seL4
support this claim: it has been formally verified for correctness~\citep{Klein_EHACDEEKNSTW_09} and
other properties~\citep{Sewell_WGMAK_11}; All memory management, including kernel memory, is all at
user-level~\citep{Elkaduwe_Derrin_06}; Finally it is the only \gls{OS} to date with full \gls{WCET}
analysis~\citep{Blackham_SCRH_11}.  The scheduler in seL4 has been left intentionally
underspecified~\citep{Petters_EH_12} for later work.  The current implementation is a placeholder,
and follows the traditional L4 scheduling model~\citep{Ruocco_06} --- a fixed-priority, round-robin
scheduler with 256 priorities.

\paragraph{\textsc{Minix 3}}~\citep{Herder_BGHT_06} is a traditional microkernel with a focus on
reliability rather than performance.  {\sc Minix 3} has been adapted for real-time with temporal
isolation provided by resource servers~\citep{Mancina_LFHGT_09}.  The implementation is designed for
bandwidth servers to be implemented at user-level.  This is achieved by adding system calls allowing
for the kernel's best-effort scheduling policy to be switched to \gls{EDF} per process.  Eight
system calls are added, as well as semantics for the kernel to up-call the resource server when a
real-time task is blocked, unblocked, exits or exhausts its budget.  Real-time tasks execute as the
second-highest priority tasks in the system, the highest being reserved for the bandwidth server.
The advantage of this implementation is that different types of bandwidth servers can be implemented
on top of {\sc Minix 3} without kernel modifications.  Although not covered in the paper, this could
be extended to a split, user-level \gls{RBED} implementation, however the overheads of such an
approach are unclear.  The {\sc Minix 3} implementation is a good example of implementing bandwidth
servers with minimal kernel modifications.  {\sc Minix 3} is not aimed at hard-real time, supporting
only mixed-criticality between \gls{SRT} and best effort processes.


\section{Other research kernels}

\paragraph{\composite} is a component-based \gls{OS} with similar with goals to a microkernel, however with a more dominant focus on support for fine-grained components.
In \composite all four mechanisms are implemented at user-level, however unlike a microkernel, \composite contains device drivers inside the kernel.

A hierarchical scheduling framework, \hires\citep{Parmer_West_11}, is used in \composite.
\hires delivers timer interrupts to a root, user-level scheduling component, which are then forwarded through the hierarchy to child schedulers.
Consequently, scheduling overhead increases as the hierarchy deepens.
Child schedulers with adequate permissions use a dedicated system call to tell the kernel to switch threads.
The kernel itself does not provide blocking semantics, which are also provided by user-level schedulers.
This design offers total scheduling policy freedom, as user-level scheduling components can implement all of the goals of a resource kernel according to their own policy.
 
% TipToe what is it and how does it schedule
\paragraph{Tiptoe}~\citep{Craciunas_KPRS_09} is a now-defunct research microkernel that also aims at temporal isolation between user-level processes and the operating system.
Like {\sc Minix 3}, Tiptoe uses the \gls{CBS} approach, although it uses variable bandwidth servers which allow for bandwidths to be altered.
Tiptoe implements bandwidth servers inside the kernel.
%TODO{look into tiptoe more detail on how it handles resource sharing}

\paragraph{Barrelfish}~\citep{Peter_SBBIHR_10} is a capability-based multi-kernel \gls{OS}, where a separate kernel runs on each processing core and kernels themselves share no memory and are essentially \emph{\gls{CPU}-drivers}.
Each CPU-driver is single-threaded and non-preemptible. 
 , although not explicitly real-time, implements a version of \gls{RBED} for managing distributed \gls{SRT} processes.
Barrelfish schedules dispatchers, which are roughly equivalent to processes. 
Execution context and scheduling context are not split.
When messages are sent between dispatchers on barrelfish the sender can choose to yield to the receiver or not by flags on the message.

TODO Quest first, then quest v. 
As a separation kernel, Quest does not provide resource sharing mechanisms by definition. 

\paragraph{Quest-V}~\citep{Danish_LW_11} is a separation kernel / hypervisor with fixed priority scheduling.
Temporal isolation is provided by sporadic servers, however \IO and normal processes are
distinguished statically: \IO processes use polling servers and normal processes use sporadic
servers, with a maximum replenishment length of 32. In Quest-V, separate partitions are assigned to
different priority levels, and are not permitted to communicate. 

\subsection{Commercial RTOSes}
%TODO add details on resource sharing to this section
There are several widely deployed \glspl{RTOS} used commercially.  The majority implement part or
all of \gls{POSIX}.  We present three popular alternatives: VxWorks, QNX Neutrino and RTEMS.

\paragraph{QNX Neutrino}~\citep{QNX_10} is a commercial, microkernel-based separation kernel that provides
\gls{FP} scheduling and resource sharing with POSIX semantics.  QNX satisfies many industry
certification standards, although these in practice do not require {\gls{WCET}} analysis or formal
verification of correctness.

\paragraph{VxWorks}~\citep{VxWorks_08} is a monolithic \gls{RTOS} deployed most notably in aircraft
and spacecraft.  It supports \gls{FP} scheduling with a native POSIX-compliant scheduler.  VxWorks
also has a pluggable scheduler framework, allowing developers to implement their own, in-kernel
scheduler.

\paragraph{PikeOS}~\citep{PikeOS:URL} implements ARINC 653~\citep{ARINC653} via a separation
microkernel which paravirtualises RTOS guests in different partitions.

There are many other \gls{RTOS}es used commercially, including \citet{Deos:URL} but the general pattern is POSIX-compliant, \gls{FP} scheduling and resource sharing.
This brief survey shows that \gls{FP} scheduling is dominant in industry due to its place in the POSIX standard. 
Although temporal isolation is sometimes provided with the possibility of bounded bandwidth via
\glspl{CBS}, asymmetric protection is not provided.  In addition to scheduling, code-bases are
generally large and complex, and beyond the grasp of modern {\gls{WCET}} analysis.  Although all of
these \gls{RTOS}es are deployed in safety critical systems, their support for mixed-criticality
applications is questionable if non-existent. 



\subsection{Summary}

%TODO{what did we learn in this chapter?}
%TODO{note gap between rt theory and systems}
%TODO{note the false correlation of priority, rt model, trust, criticality etc}

\begin{table}
\centering
\rowcolors{3}{}{gray!25}
\begin{tabular}{lllll}\toprule
  \emph{OS} & \emph{Scheduler}  & \emph{Isolation} & \emph{Asymmetric} & \emph{Resource}\\
            &                   &                  & \emph{protection} & \emph{Sharing}\\\midrule
Linux       & \gls{FP} + \gls{EDF} & \gls{CBS}          & \no  & \gls{PIP}, \gls{HLP}\\
POSIX       & \gls{FP}             & \gls{SS}           & \no  & \gls{PIP}, \gls{HLP} \\
Linux/RK    & TODO                 & TODO               & \no  & TODO \\
RTEMS       & \gls{FP} + \gls{EDF} & \gls{CBS}          & \no  & \gls{PIP}, \gls{HLP}\\
QNX         & \gls{FP}             & \gls{SS}           & \no  & TODO \\
VxWorks     & \gls{FP}             & TODO               & \no  & TODO  \\
Real-Time Mach & \gls{FP}          & \gls{DS}           & \no  & \gls{PIP} \\
EROS        & \gls{FP}             & \gls{DS}           & \no  & \gls{PIP} \\
Minix 3     & TODO                 & TODO               & \no  & TODO \\
Tiptoe      & TODO                 & TODO               & \no  & TODO   \\
Barrelfish  & \gls{EDF}            & \gls{RBED}         & \no  & Timeslice donation   \\
DROPS       & \gls{FP}             & TODO               & \no  & TODO \\
NOVA        & \gls{FP}             & \no                 & \no  & \gls{BWI} \\
Quest-V     & \gls{FP}             & \gls{SS}           & \no  & \no \\
seL4        & \gls{FP}             & \no                 & \no  & \no \\
Composite   & user level           & TODO               & TODO & TODO \\
\bottomrule
\end{tabular}
\label{t:os-summary}
\caption{Summary of \gls{MCS} support available in existing operating systems.}
\end{table}

